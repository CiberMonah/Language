\documentclass[10pt]{report}
    \title{Решение задач методом DSL\\
	На примере символьных преобразований математических выражений}
    \author{Глисанов А.С.\\МФТИ(НИУ)}
    \date{\today}
    
    \addtolength{\topmargin}{-3cm}
    \addtolength{\textheight}{3cm}
\usepackage{fontspec}
\setmainfont{Linux Libertine O}
\setmonofont{Ubuntu Mono}
\setsansfont{Noto Sans}
\usepackage{tocloft}
\usepackage{listings}
\usepackage{polyglossia}
\setdefaultlanguage{russian}
\setotherlanguage{english}
%\usepackage{luahyphenrules}
\usepackage{multirow,bigstrut}
\usepackage{tabularx}
\usepackage{mathtools}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{color}
\usepackage{tikz}
\usepackage{hyperref}
\usetikzlibrary{trees,fit,shapes.geometric}
\usepackage[russian]{minitoc}
%\usepackage{hyperref}
\usepackage[a4paper,marginparwidth=0pt,marginparsep=0pt,vmargin=2cm,hmargin=1.5cm]{geometry}
\definecolor{darkgreen}{rgb}{0.0, 0.5, 0.0}
\lstdefinestyle{mystyle}{
   % basicstyle=\small,
    breakatwhitespace=true,         
    breaklines=true,                 
    captionpos=b,           
    keywordstyle=\color{darkgreen},         
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=4,
    stringstyle=\color{magenta},
    columns=fullflexible,
    %flexiblecolumns=true,
    language=C
}
\lstdefinelanguage{ppf}{
	morestring=[b]",
	keywordsprefix={@},
	keywordsprefix={\%},
	morestring=[b]{[}{]}
}
\lstdefinelanguage{rules}{
	morekeywords={EXPAND, D, REV, DF, CONSTUP, CFOLD, PAR, LaTeX},
	otherkeywords={=>},
	keywordsprefix={PAREN},
	morecomment=[l]{--}
}


\lstset{style=mystyle}
\def\mtctitle{Содержание}
\renewcommand\lstlistingname{Лист.}
\begin{document}

\maketitle
\begin{abstract}
В данной работе мы постараемся описать способ решения задач, в которых процедурный подход приводит к большому количеству (во многом) повторяющегося кода,
	за исключением небольших элементов (в которых и кроется различие).
	Проблема заключается в том, что эти элементы могут оказаться плохими с точки зрения параметризации (например, элементарные математические или логические операции, функции и комбинаторы), для которых на языке Си придётся делать {\em обёртки}.
Либо сама постановка задачи предполагает смену парадигмы для решения, например, на продукционную или функциональную, если речь идет о преобразовании выражений или выводе на основе правил.
В таких случаях можно воспользоваться подходящим для задачи языком, имеющим поддержку нужной парадигмы.
И, если проект достаточно большой, то это будет естественным и правильным способом решения.
Но есть ситуации, в которых вся инфраструктура, связанная с новым языком может оказаться явно излишней, есть ограничения в ресурсах, или ничего из средств разработки, кроме компилятора Си недоступно.

Здесь мы покажем, что можно в ограниченных ресурсах достаточно быстро сменить парадигму, создав свой язык с интерпретатором, на котором описание решения будет выглядеть компактным и естественным (DSL -- Domain Specific Language -- специализированный предметный язык).
Причём мы это будем делать {\em с нуля (from scratch)}, не имея в руках ничего, кроме стандартной библиотеки Си, путём последовательного приближения к нужному нам языку простыми и чёткими шагами (процесс,  напоминающий шнуровку -- bootstrap).
\end{abstract} 
\dominitoc
\tableofcontents
\thispagestyle{empty}

\chapter{Задача преобразования символьных выражений}
\minitoc
\section{Математические выражения}
Для примера задачи, обладающей сложностью, рассмотрим реализацию небольшой системы компьютерной символьной алгебры (CAS). 
Основным предметом данной программы является {\em математическое выражение}, которое состоит из переменных, функций, операторов, чисел или более сложных объектов вроде векторов или матриц.
Большинство задач компьютерной алгебры сводятся к преобразованию математических выражений.
Причём под преобразованием подразумевается не вычисление выражений (как если бы оно не имело неизвестных и требовалось только в правильном порядке применить операторы к числам),
	а именно преобразование его символьной формы, с сохранением переменных, если таковые имеются\cite{wolfram, maxima}.
	К такого рода аналитическим преобразованиям относятся, например, 
		факторизация (разложение на множители), упрощение, сокращение, дифференцирование, интегрирование, замена переменных, перевод из тригонометрической формы в экспоненциальную и наоборот и т.п.
	Стоит заметить, что более сложные математические объекты, такие как комплексные числа, векторы и матрицы также имеют свой набор операторов, которые подчиняются своим правилам преобразования.

В данной работе мы ограничимся дифференцированием скалярных величин и, возможно, некоторым упрощением выражения.
Но фундамент, который будет заложен для решения этой задачи будет достаточным и для дальнейшего расширения функциональности.

\subsection{Грамматика}
Рассмотрим обычное математическое выражение, понятное выпускнику 11-го класса:
\begin{equation}
\label{expr}
	-\sin\left(\frac{2\pi k}{6}\right) +\frac{(x+3y)^2-1}{(3x-y)^3+2}\sqrt{-x}
\end{equation}
Здесь представлены основные операции и объекты школьной алгебры: Бинарные операторы сложения, вычитания, умножения, деления, возведения в степень, переменные, функции и специальные константы.
Более того, они собраны в некоторую структуру, которая указывает нам на порядок применения операторов к объектам.
В данном примере, дополнительным элементом структурирования (упорядочивания) являются скобки, которые, в тоже время, не являются ни объектом, ни оператором, но присутствуют лишь для указания порядка применения операторов.
Если бы нас попросили расписать вычисления данного выражения по действиям, мы бы без особого труда сделали бы это.
Напрмер, вас ни сколько не смутило бы выражение
$$
	2+2\cdot 2
$$
Зная ещё с начальных классов школы {\em приоритет} оператора умножения над сложением, вы получите ответ $6$ (а не $8$, если бы выполнили эти действия буквально на калькуляторе).
Таже, вычисляя 
$$
	2-1+3
$$
вы получите $4$, а не $-2$, поскольку знаете, что применение операторов сложения и вычитания осуществляется слева направо. Скобки нам нужны в более сложных случаях, либо если мы хотим нарушить естественный для нас порядок выполнения операций.

Когда мы хотим работать с системой компьютерной алгебры, мы справедливо ожидаем от нее похожего поведения в отношении расстановки приоритетов операторов, 
	дабы сократить, по крайней мере,
	количество указаний последовательности в виде доброй порции скобок.

Конечно, <<рисование>> дробей в виде двумерной структуры доступно далеко не всегда, и мы можем обойтись символьной записью деления в строчку (при помощи символа \texttt{'/'} или \texttt{':'}).
Да и специальные символы для оператора извлечения корня также далеко не универсальны. 
	Но, памятуя о том, что корень -- это, всё-таки, функция, можно воспользоваться синтаксисом функции \texttt{sqrt()} и не сильно пострадать.
В общем, с этими неудобствами вполне можно смириться ради того, чтобы остаться в рамках ультра-универсальной и простой ASCII-строки.
В конце концов, эргономика вот такой формы выражения \eqref{expr}:
\begin{verbatim}
	-sin(2*pi/6) + ((x+3*y)^2-1)/(3*x - y)^3+2)*sqrt(-x)
\end{verbatim}
не сильно уступает оригиналу. Хотя, безусловно, <<двумерный>> вариант со знакомыми спец-символами, нагляднее.

Так или иначе, мы получаем посимвольную {\em запись} математического выражения, которое подчиняется {\em правилам записи} или составления.
Правила составления выражения из символов называются {\em грамматикой}.

Запишем грамматические правила так, чтобы они образовывали {\em право-рекурсивную} PEG-грамматику\cite{ford2004}, для которой можно достаточно просто построить {\em анализатор} или {\em парсер}:

\begin{equation}\label{gram1}
	\begin{array}{rll}
		E \gets &  T (('+'|'-') T) * & \mathrm{Bыражениe} \\
		T \gets &  F (('*'|'/') F) * & \mathrm{Слагаемое} \\
		F \gets &  A ('\wedge' A) * & \mathrm{Множитель} \\
		A \gets &  N | C | V | P & \mathrm{Атом} \\
		N \gets & [0-9]+ ('.' [0-9]*))? & \mathrm{Число} \\
		V \gets & [a-z]+ & \mathrm{Переменная} \\
		C \gets & [a-z]+ '(' E ')' & \mathrm{Функция} \\
		P \gets & '(' E ')' & \mathrm{Скобочное\; выражение} 
	\end{array}
\end{equation}
Имеет смысл пояснить, что данные правила записаны в Расширенной форме Бекуса-Наура, которая предлагает дополнительные компактификации записи правил, помимо альтернативы $|$: повторения $+$ и $*$, опциональный элемент $?$.
Данная грамматика является право-рекурсивной, поскольку не содержит правил, в которых правая часть начиналась бы с нетерминала левой части.
Также она является $LL(k)$ - грамматикой\cite{llk}, поскольку, если раскрыть все определения нетерминалов и оставить только альтернативы, то выбор альтернативы возможен по конечному числу символов.

\subsection{Абстрактное синтаксическое дерево (AST)}
PEG-грамматика предполагает единственность дерева разбора, в узлах которого содержатся нетерминалы, а листьями являются терминальные символы.
	Такое дерево называется {\em Деревом синтаксического разбора}. 
Например, выражение 
\begin{equation}\label{eq:sample-expr}
	3*a + b/7-c\wedge{}2
\end{equation}
в грамматической форме \eqref{gram1} будет иметь вид синтаксического дерева, показанного на \ref{fig1}.
\begin{figure}[b]
	\begin{center}
		\begin{tikzpicture}[
		  tlabel/.style={pos=0.4,right=-1pt,font=\footnotesize\color{red!70!black}},
		 level 1/.style={sibling distance=3cm},
			level 2/.style={sibling distance=1.2cm}, 
			level 3/.style={sibling distance=1cm}, 
			level distance=0.7cm
		]
		\node{E}
			child {node{T} 
				child { node{F}
					child {node {A}
						child {node {N}
							child { node {'3'} }}}}
				child { node {'*'} }
				child { node {F}
					child { node {A}
						child {node {V}
							child { node {'a'} }}}}}
			child {node{'+'}}
			child {node{T}
				child { node{F}
					child {node {A}
						child {node {V}
							child {node {'b'}}}}}
				child { node{'/'}}
				child { node {F}
					child { node{A}
						child { node {N}
							child {node {'7'}}}}}}
			child { node {'-'}}
			child {node {T}
				child { node{F}
					child {node {A}
						child { node {V} child {node {'c'}}}}
					child {node {'$\wedge$'}}
					child {node {A}
						child { node {N} child {node {'2'}}}}}};
		\end{tikzpicture}
	\end{center}
\caption{Дерево синтаксического разбора по правилам грамматики \eqref{gram1}}
\label{fig1}
\end{figure}
Однако, это немного не то, что нам бы хотелось получить.
Мы ожидаем от парсера получить не грамматический разбор, а что-то очень приближенное к {\em дереву вычислений}, показанному на рис.~\ref{fig2}.
Вот такое дерево называется Абстрактным синтаксическим деревом (AST). 
\begin{figure}
	\begin{center}
		\begin{tikzpicture}[
			level 1/.style={sibling distance=3cm},
			level 2/.style={sibling distance=1cm},
			level 3/.style={sibling distance=0.5cm},
			level distance=0.7cm
		]
		\node{-}
			child {node{+}
				child { node{*}
					child {node {3}}
					child {node {a}}}
				child {node{/}
					child {node {b}}
					child {node {7}}}}
			child {node {$\wedge$}
				child {node {c}}
				child {node {2}}};
		\end{tikzpicture}
	\end{center}
\caption{Абстрактное синтаксическое дерево выражений (AST)}
\label{fig2}
\end{figure}

Собственно, нашей (промежуточной) целью является получение Дерева вычислений для дальнейших преобразований. 

\section{Борьба со сложностью}
Прежде, чем приступить к реализации нашей микро-CAS, на Си, имеет смысл пройтись по методам решения сложных задач
и тем инструментам, которые имеются в нашем распоряжении.

\subsection{Виды сложностей}
Сложности бывают разные. Когда мы видим перед собой гладкую высокую стену, и говорим, что будет {\em сложно} взобраться по ней, то подразумеваем {\em поведенческую сложность}, поскольку представляем себе сложный процесс --  нам потребуется где-то достать экипировку, оборудование, установить его, произвести подготовку, организовать взаимодействие команд и т.п.

Если мы смотрим на схему двигательного агрегата и говорим, что он {\em сложный}, то имеем в виду {\em структурную сложность}, поскольку мы представляем большое количество деталей, составляющих агрегат, их взаимное расположение и взаимодействие.

Если мы пытаемся запомнить музыкальное произведение или стихотворение и говорим, что оно {\em сложное}, то, скорее мы подразумеваем {\em комбинаторную сложность} -- мало повторений, уникальные словосочетания, редкие выражения или ритмический рисунок.

Когда мы  изучаем алгоритм и говорим, что он сложный, мы уточняем, что он слишком долго работает для данного объема входных данных или занимает много памяти. Т.е., говорим об алгоритмической сложности по времени или по памяти.

А можем посмотреть на код алгоритма и удивиться его {\em цикломатической сложности}\cite{cyclomatic} -- количеству ветвлений или переходов, {\em концептуальной сложности} -- количеству абстракций, и, в конце концов, {\em описательной сложности} -- общему размеру кода. 

Те виды сложностей, которые относятся к алгоритмам вполне формализованы, их  можно измерить.
Прочие же виды сложностей -- достаточно интуитивны и построение метрики для них сопряжено с некоторым произволом.

\subsection{Общие принципы}
Начнем с того, что {\em сложность}, какая бы она ни была, имеет тенденцию {\bf сохраняться} (при условии сохранения параметров задачи), возможно, перетекая из одного вида в другой, как {\em энергия}.
Поэтому, когда мы начинаем борьбу со сложностью, стоит помнить о том, что мы просто заменяем одну, неприятную нам сложность, на другую, возможно, более приятную или известную, или привычную, или не нашу.
Редка та удача, когда у нас получается {\em бесплатно} редуцировать сложность.

Теперь рассмотрим, какие методы применимы в борьбе с большинством типов сложностей (а точнее, в вытеснении или замены их другими сложностями).
\paragraph{Декомпозиция.} Принцип <<разделяй и властвуй>>. Найдите тонкие места сопряжений частей, группы сильно разнесенных элементов, группы сильно связанных элементов или плотно взаимодействующий элементов. Если речь идет о процедурах --- выделите группы действий, объединенных общей или промежуточной целью. Проведите по ним границу и рассматривайте отдельно, как относительно самостоятельные подсистемы, которым, в свою очередь, можно повторно применить {\em декомпозицию}.
\paragraph{Абстракция.} Принцип <<отсекай детали, выделяй главное>>. Старайтесь заметить общее среди множества частных и выделить его. Это могут быть либо в целом повторяющиеся действия, но в частном чуть различающиеся, либо в целом одинаковые элементы и в частном имеющие несущественные различия. Общее выделяется в {\em абстракцию}, а детали либо параметризуются, либо опускаются.
\subsection{Идиоматические методы}
К {\em идиоматическим методам} относятся те средства, которые предоставляет сам язык программирования. 
В языке Си существует возможность борьбы с процедурной сложностью путём декомпозиции участков кода и выделение их в отдельные функции.
Также Си предоставляет ограниченного рода абстракции, призванные сократить описательную или комбинаторную сложности. Во-первых, это функциональные абстракции -- обобщения поведения в виде функции и параметризация деталей поведения, в т.ч., в виде передачи ей другой функции.
Во-вторых -- это система типов, которая позволяет строить сложные типы данных (структуры и объединения) для работы с данными, содержащими множество деталей как с единым целым.
\subsection{Трансцендентные методы}
К {\em трансцендентным методам} борьбы со сложностью относят такие методы, которые нельзя реализовать в рамках самого языка, либо их реализация не приводит к сокращению нужного вида сложности. 
Например, отсутствие поддержки анонимных (лямбда-) функций в Си и замыканий резко ограничивает возможность создания абстракций поведения, поскольку, каждую функцию необходимо декларировать (давать ей имя, описывать сигнатуру) -- и это привносит дополнительную описательную сложность и сэкономить не получается.

Можно на Си сэмулировать и замыкание, и объект, и виртуальное наследование (знакомые нам по другим языкам программирования), но описательная сложность работы с такими абстракциями также может превысить ожидаемую выгоду в процедурной, цикломатической или комбинаторной сложности.

Есть и обратная ситуация, когда за минимальной описательной сложностью программы на высокоуровневом языке программирования стоит невообразимая временн\'ая сложность или потребление памяти из-за скрытых {\em накладных расходов} на реализацию языка.

В этих случаях приходится, фактически, менять язык или {\em парадигму}, или разделять реализацию на модули, каждый из которых может быть написан на своем языке программирования, более подходящим под задачу, минимизирующим требуемую сложность.

\subsection{Виды сложностей, адресуемые в различных ЯП}
Например, Си хорош тем, что имеет простую модель (множество байт с линейной адресацией), элементарные средства абстракции и из-за этого чрезвычайно быстр, поскольку модель сильнее всего приближена к конечному исполнителю (процессору) и имеет минимальную концептуальную сложность.
Си позволяет очень хорошо контролировать алгоритмическую сложность по времени и по памяти. Но совершенно стеснён в средствах борьбы со структурной сложностью, в представлении декларативной информации.

Язык Scheme хорош тем, что данные и код на нем выглядят одинаково (в виде S-выражений), имеет развитую систему макросов, анонимные функции, замыкания, продолжения (continuation). Это позволяет контролировать описательную, процедурную и структурную сложности (декларатив).
Более того, Поскольку выражения языка и сами являются и отражают элементы AST, создание новых языков делается на Scheme проще всего.
Но, из-за отсутствия строгой типизации и большого количества накладных расходов, не может соперничать с Си по скорости выполнения программ и по потреблению памяти.
 
Язык Python имеет более развитые средства абстракций, сокращающие описательную сложность, правда, отличающиеся от тех, которые предоставляет Scheme. Например, гомоиконичность в Python отсутствует, зато есть возможность назначать свою {\em семантику} различным синтаксическим элементам (скобкам и прочим операторам).
Есть абстракции объектно-ориентированного подхода, лямбда-функции, 
Но, естественно, платить за это приходиться сложностью  интерпретатора и временем исполнения программ.
\subsection{Микро-ФОРТ и нано-ЛИСП спешат на помощь}
\chapter{Пример программы}
\minitoc
\section{Обзор архитектуры}
Для начала, опишем общую архитектуру программы.
Начнём сверху -- с результата -- и дальше будем двигаться вниз к деталям реализации.

Если мы хотим научить программу преобразовывать математические выражения, то нам больше всего подойдет парадигма логического или продукционного подхода, когда программа описывается в виде множества {\em продукций} --- правил вида ЕСЛИ-ТО. 
Поскольку мы собираемся иметь дело не с разрозненными фактами, а синтаксической структурой, то естественным требованием будет возможность {\em сопоставления с образцом} -- pattern-matching. 

Таким образом, на <<самом верху>> нас ждёт некоторая система обработки правил $\boxed{Prod}$, которая, помимо самих правил $R$, принимает на вход синтаксическую структуру $T_E$ и выдает также (модифицированную) AST $T'_E$.
$$
\begin{array}{ccc}
	T_E \rightarrow & \boxed{Prod} & \rightarrow T'_E \\
	                &  \uparrow & \\
	                & R & 
\end{array}
$$
Правила $Rules$ в виде серий \texttt{if/else} или \texttt{switch/case} можно записывать и на Си, но лучше бы это было делать в более подходящем виде, например в виде выражений:
\begin{equation}\label{eq:best-form}
\begin{array}{rl}
\mathrm{Diff}(A + B) & \Rightarrow  \mathrm{Diff}(A) + \mathrm{Diff}(B) \\
\mathrm{Diff}(A \cdot B) & \Rightarrow \mathrm{Diff}(A)\cdot B + \mathrm{Diff}(B)\cdot A \\
\ldots & \Rightarrow \ldots
\end{array}
\end{equation}
Вместо, например, следующего кода:
\begin{lstlisting}
Expr* diff(Expr* in) {
	switch(in->type) {
		case BINOP:
			if(in->op == '+') {
				return new_expr(BINOP, '+', diff(in->left), diff(in->right));
			}
			...
		}
		...
	}
}
\end{lstlisting}
или подобного.
Поскольку таких правил может быть внушительное количество, хотелось бы уменьшить описательную сложность, путем редукции повторяющихся конструкций.
 Т.е. было бы неплохо иметь язык описания правил, синтаксис которого, возможно, отличался бы от синтаксиса языка выражений.
 Более того, наверняка нам потребуются правила для редукции констант и прочих модификаций выражения.
  Таким образом, правила $R$ должны браться из файла с правилами, анализироваться, аналогично математическим выражениям, и исполняться (см. рис.~\ref{fig:prod}).
\begin{figure}[b]
\begin{center}
\begin{tikzpicture}
\node (expr) at (-1,0) {$E$};
\node[draw] (parser)  at (1,0) {$Parser$};
\node (exprgr) at (1,-1) {$G_E$} edge [->] (parser);
\node (ast1) at (3, 0) {$T_E$};
\node[draw] (prod) at (5, 0) {$Prod$};
\node (rules) at (5, -1) {$Rules$};
\node[draw] (parser2) at (5, -2) {$Parser$};
\node (rulesf) at (3, -2) {$F_R$};
\node (rulesg) at (5, -3) {$G_R$};
\node (ast2) at (7, 0) {$T'_E$}; 
\path [->] (expr) edge (parser)
			(parser) edge (ast1)
			(ast1) edge (prod)
			(rules) edge (prod)
			(rulesf) edge (parser2)
			(parser2) edge (rules)
			(rulesg) edge (parser2)
			(prod) edge (ast2);
\end{tikzpicture}
\caption{Общая архитектура программы}
\label{fig:prod}
\end{center}
\end{figure}
Здесь у нас появляется некоторый универсальный элемент $\boxed{Parser}$, который может принимать на вход описание грамматики и разбирать выражения $E$. 
В нашем случае -- это грамматика математических выражений $G_E$ и грамматика правил $G_R$.
Сами правила могут храниться в отдельном файле $F_R$.

\section{Элементарный packrat-парсер}
Packrat-парсер -- это реализация анализатора строк на основе PEG-грамматики. 
PEG-грамматика позволяет описывать правила парсинга каждого нетерминала в виде выражений, построенных на основе терминалов, ссылок на нетерминалы  c использованием связок.

Терминальными элементами грамматики являются следующие:
\begin{itemize}
\item {\em литерал} (literal) -- последовательность символов (строка). В описании правил обычно обозначается в виде строки в кавычках: \texttt{'some literal'}.
\item {\em один-из} (oneof) -- читаемый символ совпадает с одним из заданных, обычно задается в виде набора в квадратных скобках: \texttt{[aeoui]}.
\item {\em диапазон} (range)  -- читаемый символ принадлежит одному из диапазона, обычно задается в виде пары символов, указанных через дефис в квадратных скобках: \texttt{[a-z]}. Чаще всего, эти два типа шаблона совмещаются в один: \texttt{[a-z,+-/*]}.
\item {\em любой} (any) -- читаемый символ может быть любым -- просто точка \texttt{.}.
\item {\em нетерминал} -- ожидается строка, удовлетворяющая указанному нетерминалу --- имя нетерминала без кавычек.
\end{itemize}

Связки, поддерживаемые PEG, могут быть следующими:
\begin{itemize}
\item {\em последовательность} (sequence) -- пара элементов стоят последовательно, обычно специального символа для этого не требуется: \texttt{e1 е2}.
\item {\em альтернатива или выбор} (choice) -- анализ может пойти по одному из путей $e_1 | e_2$. Для обозначения используется либо символ pipe:  \texttt{e1 | e2} , либо слэш: \texttt{e1 / e2}. В целях избежания неоднозначности, последовательность имеет значение. Альтернативные шаблоны проверяются в указанном порядке.
\item {\em повторения ноль-или-больше, один-или-больше} (zero-or-more, one-or-more) -- элемент может быть повторен несколько раз. Используется символ '*' если количество повторений может быть нулевым: \texttt{e *}, и символ '+', если требуется хотя бы одно повторение: \texttt{e +}.
\item {\em опциональный элемент} (optional) -- элемент может присутствовать, а может и не присутствовать в строке. Обозначается символом '?' после элемента: \texttt{e ?}.
\end{itemize}

Дополнительные предикаты:
\begin{itemize}
\item {\em и-предикат} (and-predicate) -- ранняя проверка (ahead-lookup), что строка удовлетворяет шаблону, но при этом строка не потребляется.  Обозначается символом \texttt{\&} перед элементом: \texttt{\& e}.
\item {\em не-предикат} {not-predicate}  -- проверка, что шаблон не содержится в строке. Обозначается символом \texttt{!} перед элементом \texttt{! e}. 
\end{itemize}

Пример описания синтаксиса языка математических выражений с использованием PEG-грамматики:
\begin{verbatim}
   expr <- term ( '+' / '-' term) *
   term <- factor ( '*' / '/' factor) *
   factor <- atom ( '^' atom )*
   atom <- func / var / num / '(' expr ')'
   func <- name '(' args? ')' 
   var <- name
   num <- [0-9] + ('.' [0-9]*) ? 
   name <- [a-Z]+
   args <- expr (',' expr) *
\end{verbatim}

Опять же, чтобы не записывать PEG-грамматику в виде комбинаций шаблонов на Си, а в формате, близком к PEG, 
нам снова потребуется $\boxed{Parser}$ и \textbf{круг замыкается}. 

Для разрешения циклической зависимости парсера от парсера существуют два пути: либо написать изначальный парсер PEG-грамматики целиком на Си, либо взять некоторый простой прото-язык, парсер которого пишется достаточно просто, и при помощи него создавать нужную грамматику.

\subsection{Микро-ФОРТ и бинарное дерево для описания структур}
Если соизмерять сложность написания парсера PEG-грамматик на чистом Си с чем-то другим, то имеет смысл рассмотреть сначала что-то всё-таки более простое.
В качестве примера языка с элементарным синтаксисом можно взять ФОРТ.
Это простой, конкатенативный язык со стековой моделью памяти.
Нам от ФОРТа нужны только его примитивный синтаксис -- это просто последовательность лексем (слов), не содержащих пробелов и стековая модель, которая определяется (или индуцирует) обратную польскую запись.
Парсер и интерпретатор ФОРТа пишутся очень быстро.

В качестве же элементарного строительного элемента, нам подойдет {\em пара} (pair), так что все возможные структуры будут представляться в виде {\em бинарного дерева}.
Например, синтаксическая структура правила вывода \texttt{expr} языка математических выражений будет выглядеть следующим образом (см. рис.~\ref{binary-grammar}).
\begin{figure}[b]
\begin{center}
		\begin{tikzpicture}[
			level 1/.style={sibling distance=2cm},
			level 2/.style={sibling distance=1cm},
			level 3/.style={sibling distance=1cm},
			level distance=0.7cm
		]
		\node{;}
			child {node{\texttt{term}}}
			child {node{*}
				child { node{;} 
					child { node {|} 
						child { node{\texttt{'+'}}}
						child { node{\texttt{'-'}}}}
					child { node{\texttt{term}}}}};
		\end{tikzpicture}
	\end{center}
\caption{Синтаксическое дерево грамматического правила \texttt{expr <- term ('+' / '-' term ) *}}
\label{binary-grammar}
\end{figure}
Здесь нам пришлось в явном виде задать оператор создания последовательности (sequence) -- в узлах он обозначен как \texttt{';'}.

Особенностью обратной польской записи является отсутствие необходимости указаний скобок для группировки.
Поэтому в нашем варианте ФОРТ-а построение такого дерева будет выглядеть вот так:
\begin{verbatim}
   term '+' '-' / term ; * ;
\end{verbatim}

Таким образом, наш модуль $\boxed{Parser}$ будет состоять из следующих компонент (см. рис.~\ref{fig:parser}).
\begin{figure}[b]
\begin{center}
\begin{tikzpicture}
\node (str) at (0,0) {$String$};
\node[draw] (packrat) at (2, 0) {$Packrat$};
\node (gr) at (2, -1) {$Grammar$};
\node[draw] (uforth) at (2,-2) {$\mu Forth$};
\node (ast) at (4, 0) {$Match$};
\node (gf) at (2, -3) {.fgf $File$};
\path [->] (str) edge (packrat)
			(packrat) edge (ast)
			(gr) edge (packrat)
			(uforth) edge (gr)
			(gf) edge (uforth);
\node[draw,inner sep=4pt, thick, rectangle, fit=(packrat) (gr) (uforth)] {};
\end{tikzpicture}
\caption{Модуль $\boxed{Parser}$ внутри}
\label{fig:parser}
\end{center}
\end{figure}
Модуль $\boxed{\mu Forth}$ читает грамматические правила из файла --- пусть этот файл будет иметь расширение {\tt .ppf}, --- и создает структуру грамматики (см.~\ref{sec:grammar} и \ref{sec:matchers}).
Затем, эта структура отправляется packrat-парсер (\ref{sec:packrat}) для поиска совпадений.

\subsection{Грамматика}
\label{sec:grammar}
Структура Грамматики \texttt{Grammar} задается в файле \texttt{packrat.c}:
\begin{lstlisting}[firstnumber=7]
struct _grammar {
	Rule* rules;
	Matcher* allmatchers;
	int allocated;
};
\end{lstlisting}
Состоит из массива правил \texttt{rules} и массива всех шаблонов \texttt{allmatchers}.
Это поле нужно для удобства аллокации и освобождения структур шаблонов.
Каждое правило из массива \texttt{rules} ссылается на некоторый корневой шаблон из массива \texttt{allmatchers}, он, уже, в свою очередь, на свои дочерние элементы и т.д. 
Поле \texttt{allocated} хранит бит, указывающий на происхождение структуры грамматики.
Она может быть задана как в статическом виде, средствами языка Си, так и динамически при парсинге  грамматических правил на ФОРТе.

\subsection{Структура шаблонов}
\label{sec:matchers}
Каждый элементарный шаблон описывается структурой \texttt{Matcher}. Она является общей для нескольких модулей, поэтому описывается в заголовочном файле \texttt{packrat.h}.
Шаблон может иметь опциональное имя \texttt{name}, Указатель на функцию сопоставления \texttt{func} (см.~\ref{pattern-matchers}),
Количество потомков (арность) указывается в поле \texttt{args}.
 Для связок типа {\em последовательность} и {\em альтернатива} таких потомков 2, 
для терминальных шаблонов типа {\em один-из}, {\em литерал}, {\em диапазон} и {\em любой символ} -- 0, 
для прочих шаблонов -- 1.

Параметры шаблона, например, имя нетерминала, диапазон или литерал  указываются в поле \texttt{opt}.

Указатели на дочерние шаблоны хранятся в полях \texttt{left} и \texttt{right}.
Если шаблон является ссылкой на нетерминал, то эта ссылка записывается в поле \texttt{ref}.
\begin{lstlisting}[firstnumber=6]
typedef struct _matcher {
	char* name;
	struct _match* (*func)(struct _matcher*, const char*);
	int args;
	char* opt;
	int n;
	struct _matcher* left;
	struct _matcher* right;
	struct _rule* ref;
	struct _grammar* g;
	struct _matcher* next;
	Table* cache;
} Matcher;
\end{lstlisting}
Далее следуют служебные поля, которые требуются для удобства: \texttt{g} -- указатель на родительскую грамматику, \texttt{next} -- указатель на следующей элемент на стеке (нужен при сборке грамматики из описания на ФОРТ-е).
Поле \texttt{cache} -- указатель на хеш-таблицу результатов данного шаблона (ключевая часть packrat-парсера, см.~\ref{caching}).

\subsection{Кеширование}
\label{caching}
Кеширование результатов матчинга осуществляется в хеш-таблице (см.~\ref{sec:hash-table}). 
Любая проверка шаблона проходит через функцию-обёртку {\tt do\_match}:
\begin{lstlisting}[firstnumber=63]
/* match wrapper with auto cache */
Match* do_match(Matcher* m, const char* txt) {
	cnt ++;
	Match* res;
	//printf("Trying to parse \"%.10s\" with matcher %p(%d)\n", txt, m, m->n);
	if ( ht_get(m->cache, (void*)txt, (void**)&res) ) return res;
	res = m->func(m, txt);
	ht_set(m->cache, (void*)txt, res, res && res->m == m?_match_free:NULL);
	return res;
}
\end{lstlisting}
В ней проверяется наличие результата матчинга в кеше ({\tt ht\_get}) по ключу, которым является указатель на текущую позицию в строке {\tt txt}.
Если результат {\tt res} для данного шаблона {\tt m} и позиции {\tt txt} существует, то возвращается он и повторного матчинга не производится.
В противном случае вызывается функция матчера {\tt m->func} и результат сохраняется в хеш-таблице ({\tt ht\_set}). 

Стоит заметить, что в случае, если результат был сгенерирован дочерним матчером ({\tt res->m != m}), то функция освобождения передается нулевая ({\tt NULL}), в противном случае, используется {\tt \_match\_free} (см.~\ref{match-aux}). 

\subsection{Элементарные шаблоны и их композиция}
\label{pattern-matchers}
Далее идёт перечисление функций-матчеров терминальных шаблонов и композиций.
Каждый матчер принимает на вход, собственно, структуру шаблона {\tt m} и указатель на текущий символ в строке {\tt txt}. 
Возвращает либо {\em Совпадение} {\tt Match}, либо {\tt NULL}.
Каждое совпадение -- это структура, описанная в файле {\tt packrat.h}:
\begin{lstlisting}[firstnumber=20]
typedef struct _match {
	Matcher* m;
	const char* start;
	const char* end;
	struct _match* left;
	struct _match* right;
} Match;
\end{lstlisting}
Она содержит указатель на шаблон {\tt m}, указатель на начало и конец подстроки, удовлетворяющей шаблону {\tt start, end}, а также на совпадения дочерних матчеров, которые могу быть указаны в полях {\tt left} и {\tt right}.
Поле {\tt end} указывает на символ, {\em следующий} за последним символом совпадения.

Создание Совпадений происходит через функцию-конструктор {\tt new\_match}:
\begin{lstlisting}[firstnumber=35]
Match* new_match(struct _matcher* m, const char* start, const char* end, Match* l, Match* r) {
	Match* n = malloc(sizeof(Match));
	*n = (Match){m, start, end, l, r};
	return n;
}
\end{lstlisting}

Теперь опишем все известные матчеры.
\paragraph{Матчер последовательности} {\tt seq\_match}. Проверяет, что левый и правый дочерние матчеры вернули совпадения.
Создает новое совпадение, которое объединяет оба дочерних. Правый матчер начинает разбор с конца совпадения правого.
\begin{lstlisting}[firstnumber=74]
Match* seq_match(Matcher* m, const char* txt) {
	Match* ml = do_match(m->left, txt);
	if ( ml ) {
		Match* mr = do_match(m->right, ml->end);
		if ( mr ) return new_match(m, txt, mr->end, ml, mr);
	} 
	return NULL;
}
\end{lstlisting}

\paragraph{Матчер выбора} {\tt or\_match}. Если сработал шаблон левой альтернативы, то возвращается его совпадение, в противном случае возвращается то совпадение, которое вернёт правый шаблон.
\begin{lstlisting}[firstnumber=83]
Match* or_match(Matcher* m, const char* txt) {
	Match* ml = do_match(m->left, txt);
	return ml ? ml : do_match(m->right, txt);
}
\end{lstlisting}

\paragraph{Матчер НЕ-предиката} {\tt not\_match}. Возвращает совпадение нулевого размера, если дочерний шаблон (указан в {\tt left}) не совпал и {\tt NULL} в случае совпадения. Указатель на текущий символ при этом не смещается.
\begin{lstlisting}[firstnumber=88]
Match* not_match(Matcher* m, const char* txt) {
	Match* ml = do_match(m->left, txt);
	return  ml ? NULL : new_match(m, txt, txt, NULL, NULL);
}
\end{lstlisting}

\paragraph{Опциональный матчинг} {\tt optional\_match}. В любом случае возвращает совпадение. Если совпал дочерний шаблон, то возвращает его совпадение, если нет -- возвращает совпадение нулевого размера.
\begin{lstlisting}[firstnumber=93]
Match* optional_match(Matcher* m, const char* txt) {
	Match* ml = do_match(m->left, txt);
	return ml ? ml : new_match(m, txt, txt, NULL, NULL);
}
\end{lstlisting}
\paragraph{Матчер И-предиката}, или раннее обнаружение {\tt ahead\_match}. Возвращает совпадение нулевого размера, если дочерний шаблон (заданный в поле {\tt left}) присутствует в строке. Позицию текущего символа, соответственно, не сдвигает.
Т.е. как-бы смотрит вперёд и ищет совпадение заранее.
\begin{lstlisting}[firstnumber=108]
Match* ahead_match(Matcher* m, const char* txt) {
	Match* ml = do_match(m->left, txt);
	return ml ? new_match(m, txt, txt, NULL, NULL) : NULL;
}
\end{lstlisting}

\paragraph{Терминальный шаблон Совпадение литерала} {\tt literal\_match}. Возвращает совпадение, если указанная (в поле {\tt opt}) подстрока находится на позиции {\tt txt}. Длина совпадения соответствует длине подстроки.
\begin{lstlisting}[firstnumber=98]
Match* literal_match(Matcher* m, const char* txt) {
	return (!strncmp(txt, m->opt, strlen(m->opt)))? new_match(m, txt, txt+strlen(m->opt), NULL, NULL) : NULL;
}
\end{lstlisting}
\paragraph{Терминальный шаблон Один-Из} {\tt oneof\_match}. Возвращает совпадение размером в один символ, если символ из списка, заданного в виде строки в {\tt opt}  находится на позиции {\tt txt}. 
\begin{lstlisting}[firstnumber=102]
Match* oneof_match(Matcher* m, const char* txt) {
	for ( const char* x = m->opt; *x; x++ ) 
		if (*txt == *x ) return new_match(m, txt, txt+1, NULL, NULL);
	return NULL;
}
\end{lstlisting}

\paragraph{Терминальный матчер диапазона} {\tt range\_match}. Возвращает совпадение в один символ, если символ принадлежит диапазону символов, заданном в первых двух символах строки {\tt opt}. 
\begin{lstlisting}[firstnumber=113]
Match* range_match(Matcher* m, const char* txt) {
	return (*txt >= m->opt[0] && *txt <= m->opt[1] ) ? new_match(m, txt, txt+1, NULL, NULL) : NULL;
}
\end{lstlisting}
\paragraph{Терминальный матчер любого символа} {\tt any\_match}. 
\begin{lstlisting}[firstnumber=117]
Match* any_match(Matcher* m, const char* txt) {
	return  (*txt) ? new_match(m, txt, txt+1, NULL, NULL) : NULL;
}
\end{lstlisting}
\paragraph{Матчер повторений} {\tt repeat\_match} -- Он один, реализует семантику ноль-и-больше повторений дочернего шаблона, который также передается через {\tt left}. Если дочерний шаблон найден, рекурсивно вызывает себя. Если ни один повтор не найден, возвращает <<нулевое>> совпадение.
\begin{lstlisting}[firstnumber=121]
Match* repeat_match(Matcher* m, const char* txt) {
	Match* ml = do_match(m->left, txt);
	if ( ml ) {
		Match* mr = do_match(m, ml->end);
		if ( mr ) return new_match(m, txt, mr->end, ml, mr);
	}
	return new_match(m, txt, txt, NULL, NULL);
}
\end{lstlisting}
\paragraph{Ссылочный шаблон}.  Ссылается на синтаксическое правило, указанное в поле {\tt ref} и ищет совпадение по нему.
\begin{lstlisting}[firstnumber=130]
Match* ref_match(Matcher* m, const char* txt) {
	Rule* ref = m->ref;
	return ref ? do_match(ref->assembly, txt) : NULL;
}
\end{lstlisting}


\subsection{Именованные совпадения}
Особняком стоит функция-матчер, которая ничего не делает, кроме того, что вставляет в структуру элемент, дублирующий дочерний, но  имеющий {\em имя}, которое указано в поле матчера {\tt m->name}. 
\begin{lstlisting}[firstnumber=135]
Match* named_match(Matcher* m, const char* txt) {
	Match* ml = do_match(m->left, txt);
	return ml ? new_match(m, txt, ml->end, ml, NULL) : NULL;
}
\end{lstlisting}
Имена нам нужны для того, чтобы построить AST, более приближенного к синтаксису выражений, вместо бинарного дерева порожденного нашей структурой матчеров, в которой может быть много лишнего (см.~\ref{sec:match-walk}).
\subsection{Проход по совпадениям и Генерация AST}
\label{sec:match-walk}
Для получения AST используется функция {\tt match\_walk}. 
Она принимает на вход корневое совпадение {\tt m}, функцию для обратного вызова {\tt cb} и пользовательские данные {\tt data}. 
Функция обратного вызова вызывается только на тех совпадениях которые имеют установленное (ненуливое) поле {\tt name}. Т.е., отрабатывает только на именованных совпадениях. 
Таким образом мы получаем вместо избыточного дерева совпадений абстрактное синтаксическое дерево (AST). 
\begin{lstlisting}[firstnumber=205, caption=packrat.c -- Функция обхода синтаксического дерева,label=lst:match-walk]
int _match_walk(Match* m, matchcb cb, void* data);

int match_walk(Match* m, matchcb cb, void* data) {
	if ( !m ) return 0;
	return _match_walk(m->left, cb, data) || _match_walk(m->right, cb, data);
}

int _match_walk(Match* m, matchcb cb, void* data) {
	if ( !m ) return 0;
	if ( m->m->name ) { 
		if ( cb(m, data) ) return 1;
	} else
		return match_walk(m, cb, data);
}
\end{lstlisting}
Обратный вызов получает в качестве аргументов текущее именованное совпадение {\tt m} и пользовательские данные {\tt data}, полученные ранее при вызове {\tt match\_walk}.
Чтобы получить дочерние элементы текущего именованного совпадения, достаточно рекурсивно вызвать {\tt match\_walk} на нём. Сигнатура обратного вызова {\tt match\_walk} указана в заголовочном файле {\tt packrat.h}: 
\begin{lstlisting}[firstnumber=37]
typedef int (*matchcb)(Match* m, void* data);
\end{lstlisting}
Функция обратного вызова может вернуть ненулевое значение, чтобы прекратить поиск.

\section{Микро-ФОРТ для описания грамматики}
\subsection{Структура грамматических правил}
Как видно из \ref{sec:matchers}, правила являются бинарным деревом, составленным из элементарных шаблонов (Matchers), которые, в свою очередь являются структурой, содержащей указатель на функцию-матчер, её параметры и ссылки на дочерние элементы. 
Сам набор грамматических правил является массивом структур {\tt Rule}, которая описана в {\tt packrat.h}:
\begin{lstlisting}
typedef struct _rule {
	char* name;
	Matcher* matchers;
	Matcher* assembly;
} Rule;
\end{lstlisting}
Каждое правило имеет имя {\tt name}, указатель на массив матчеров {\tt matchers} и указатель на <<собранный>> корневой матчер {\tt assembly}. Что значит <<собранный>> мы узнаем чуть ниже.

Дальше перед нами стоит задача создания данной структуры.
Подойти к этому вопросу можно двумя способами: использовать декларативные возможности языка Си, либо перейти к трансцендентным методам.

Давайте для примера рассмотрим, что нам может предложить Си для построения вот такой грамматической структуры:
\begin{equation}
\begin{array}{rl}
	Expr \gets  & Term \; (TermOp \; Term)\;* \\
	TermOp \gets & '+' \; | \; '-' \\
	Term \gets & \ldots 
\end{array} 
\end{equation}
Добавим к этому наше желание, чтобы совпадения Expr, Term и TermOp были именованными. Как бы мы статически конструировали это на Си:
\begin{lstlisting}
Matcher term = {"Term", ...};
Matcher plus_or_minus = {"TermOp", oneof_match, .opt="+-"};
Matcher term1 = {NULL, ref_match, .ref=term};
Matcher term2 = {NULL, ref_match, .ref=term};
Matcher seq1 = {NULL, seq_match, .left = plus_or_minus, .right=term2};
Matcher rep1 = {NULL, repeat_match, .left = seq1};
Matcher expr = {"Expr", seq_match, .left=term1, .right=rep1};
\end{lstlisting}
Как видим, для поддержания возможности именованных ссылок на дочерние объекты в структуре, мы вынуждены держать именованные константы.
Можно немного снизить описательную сложность, добавив некоторый простой стековый интерпретатор, который бы строил дерево не по именам элементов, а согласно их расположению на стеке, например, если требуется построить элемент с двумя дочерними элементами, то они должны находиться на вершине стека, новый родительский элемент <<подцепляет>> в свои поля {\tt left} и {\tt right}, убирает их со стека, и сам становится на вершину (см. рис.~\ref{fig:stack}).
\begin{figure}[b]
\begin{center}
\begin{tikzpicture}
\node[draw] (term1) at (0,0) {term1};
\node[draw] (rep1) at (0,-1) {rep1};
\node at (0, -3) {1)};
\node[draw, dashed] (expr) at (2, 0) {expr};
\node[draw] (term11) at (2, -1) {term1};
\node[draw] (rep11) at (2, -2) {rep1};
\node at (2, -3) {2)};
\node[draw] (expr1) at (4, 0) {expr};
\node at (4, -3) {3)};
\node[draw] (term12) at (6, 0) {term1};
\node[draw] (rep12) at (6, -1) {rep1};
\path [->] (expr1) edge (term12) edge (rep12);
\node[draw,inner sep=4pt, thick, rectangle, fit=(term1) (rep1)] {};
\node[draw,inner sep=4pt, thick, rectangle, fit=(term11) (rep11) ] {};
\node[draw,inner sep=4pt, thick, rectangle, fit=(expr1)] {};
\end{tikzpicture}
\caption{Процесс построения дерева без <<переменных>>: 1) Стек до  положения expr, 2) -- момент добавления expr, 3) expr забирает себе в дочерние term1 и rep1 и сам становится вершиной  }
\label{fig:stack}
\end{center}
\end{figure}

В принципе, для построения древовидной структуры нет необходимости каждому узлу присваивать имена,  \emph{достаточно стека}.
Иногда, правда, нам придется ссылаться на другие элементы по имении, но это будет происходить относительно редко (по сравнению с другими комбинациями) и не будет нам сильно усложнять жизнь.

Итак, стековых операций в нашем случае оказывается немного -- просто положить на стек (это будет терминальный матчер), связать новый элемент с одним элементом с вершины и положить на стек, снять и связать два элемента и положить на стек новый. Воспользуемся обратной польской нотацией и запишем все элементы в массив:
\begin{lstlisting}[caption=Массив матчеров, label=lst:matchers-array]
Matcher expr_matchers[] = {{NULL, ref_match, .ref=term},
						   {"TermOp", oneof_match, .opt="+-"},
						   {NULL, ref_match, .ref=term},
						   {NULL, seq_match, .args=2}, 
						   {NULL, rep_match, .args=1},
						   {"Expr", seq_match, .args=2},
						   {NULL, NULL}};
\end{lstlisting}
Здесь мы для обобщения ввели в оборот поле \texttt{args}, чтобы интерпретатору было проще понимать, сколько снимать со стека аргументов и добавлять в качестве дочерних новой вершине.

Также имеет смысл ограничить последовательность интерпретации некоторой структурой с особым значением, например с полем {\tt func=NULL}.

Как видим, число повторяющихся конструкций языка сильно сократилось в декларативной части.
Интерпретатор-сборщик для данного массива может выглядеть, например, так:
\begin{lstlisting}[caption=Пример сборщика из массива, label=lst:assembler-sample]
Matcher* assembly(Matcher* matchers) {
	Matcher* m;
	for ( m = matchers; m->func; m++ ) {
		switch(m->args) {
		case 1: m->left = _pop(); break;
		case 2: m->right = _pop(); m->left = _pop();  break;
		}
		push(m);
	}
	return _pop();
}
\end{lstlisting}
Сборщик правил выглядит достаточно просто, при условии реализации функций работы со стеком {\tt \_push()} и {\tt \_pop()}, и при этом экономит количество символов, требуемых на создание дерева шаблонов.

\subsection{Синтаксис ФОРТ-а -- поток токенов}
К сожалению, оставаясь в рамках Си, процент избыточных синтаксических элементов при декларативном описании будет достаточно велик, полностью редуцировать его невозможно и тут пора переключаться на новый язык.

Тут нам поможет ФОРТ, как наиболее простой с точки зрения синтаксиса. Программа на ФОРТе -- это просто поток токенов -- слов, разделенных пробельными символами. Мы воспользуемся этой идеей, но немного разовьём её.
Список токенов легко преобразуется в массив (типа~\autoref{lst:matchers-array}).

Мы сделаем так, чтобы для каждого типа матчера существовал свой токен (тип слова).
Проще всего различать тип токена по его началу -- например, если токен начинается с \verb|'"'| (кавычек), то это литерал, если с \verb|'['|, то это диапазон, если в \verb+'{'+, то это  one-of. Если начинается с \verb|'@'|, то это будет означать именованный матчер.
 Похожим образом будем отмечать и другие токены, которые бы обозначали соответствующие матчеры, путь даже состоящие из одного символа: \verb|';'| -- объединение в последовательность, \verb+'|'+ -- выбор, альтернатива, \verb|'.'| -- любой символ, \verb|'*'|  -- повторение.

Плюс к этом добавим ещё специальный вид токена, обозначающего начало нового грамматического правила для нетерминала, например, слово начинающееся на \verb+'%'+. Правило будет заканчиваться там, где начинается новое, либо в конце файла.
Так что наши грамматические правила будут иметь такой вид:
\begin{verbatim}
%EXPR TERM [+-] @TermOp TERM ; * ; @EXPR
%TERM ...   
\end{verbatim}
Думаю, тут описательная сложность будет уменьшена до практического минимума.

\subsection{Токенизатор}
Разбор токенов будем производить из файла и посвятим ему вот эти две функции-\emph{токенизаторы}, которые расположены в файле \texttt{uforth.c}: высокоуровневую {\tt tokenize} и вспомогательную {\tt \_tokenize}.
 Токеном мы будем считать не только слова, не содержащие пробелов, но если строка заключена в кавычки или скобки -- мы не будем обращать внимания на пробелы, также мы будем давать возможность вводить специальные символы через экранирование (escaping) символом \verb|'\'|. 
 Мы сделали такое отступление от классического <<слова>> ФОРТа для удобства задания литералов с пробельными символами.

Вспомогательная функция {\tt \_tokenize}, принимает на вход файл (поток) {\tt f}, указатель на буфер и максимальны размер буфера, куда будет записан токен {\tt buf} и {\tt bufsz} соответственно, а также ожидаемый конец токена {\tt waitfor}  и режим экранирования {\tt escape}. 

Для сокращения записи, мы вводим два макроса.
Первый -- {\tt ESC2SYM} -- преобразует последовательности вида \verb|'\t'|, \verb|'\n'| в соответствующие символы (табуляция и перевод строки)
Второй -- {\tt ISWHITE} -- проверяет, является ли читаемый символ пробельным (пробел, табуляция или перевод строки).

Функция {\tt \_tokenize} написана в рекурсивном стиле и читает поток посимвольно.
Если переданное  значение {\tt waitfor} равно нулю, то функция продолжает добавлять символы в буфер, пока не встретится пробельный символ, в противном случае, функция будет ожидать либо закрытия кавычек, либо скобок.
В режиме {\tt escape=1},  функция преобразует экранируемый символ в соответствующий код {\tt ESC2SYM}, выходит из режима экранирования (передаёт {\tt escape=0}) и продолжает читать поток.
 
\begin{lstlisting}[firstnumber=7, caption=uforth.c -- вспомогательный токенизатор]
#define ESC2SYM(c) ((c)=='t'?'\t':(c)=='n'?'\n':(c))
#define ISWHITE(c) ((c)==' '||(c)=='\n'||(c)=='\t')

/* simple tokenizer with support of escaping */
int _tokenize(FILE* f, char* buf, size_t bufsz, int waitfor, int escape) {
	int c = getc(f);
	if ( bufsz == 0 ) return -2; /* out of buffer */
	if (c == EOF)  return (*buf = 0, c); /* end of file */
	if ( escape ) return (*buf = ESC2SYM(c), _tokenize(f, buf+1, bufsz-1, waitfor, 0)); /* processing escape */
	else if ( waitfor && c == waitfor ) return (*buf = 0, waitfor); /* waitfor encountered */
	else if (!waitfor && ISWHITE(c)) return (*buf= 0, waitfor); /* whitespace */
	else if ( c == '\\' ) return _tokenize(f, buf, bufsz, waitfor, 1); /* entering escape mode */
	else return (*buf = c, _tokenize(f, buf+1, bufsz-1,  waitfor, 0)); /* continue filling buffer */
}
\end{lstlisting}
В случае выхода за пределы буфера, вспомогательный токенизатор возвращает код ошибки (-2).

Высокоуровневый токенизатор {\tt tokenize} просто читает поток {\tt f}, записывает в буфер, переданный в виде указателя {\tt buf} и размера {\tt bufsz} строку, соответствующую токену и возвращает {\em код токена} (который в нашем случае будет просто равен первому символу), либо {\tt EOF} в случае окончания потока или ошибки.

\begin{lstlisting}[firstnumber=21, caption=uforth.c -- высокоуровневый токенизатор]
/* read the file token-by-token */
int tokenize(FILE* f, char* buf, size_t bufsz) {
	int c = getc(f);
	switch(c) {
	case EOF: return c;
	case ' ':  case '\t': case '\n': return tokenize(f, buf, bufsz);
	case '"': return _tokenize(f, buf, bufsz, '"', 0)=='"'?'"':EOF;
	case '{': return _tokenize(f, buf, bufsz, '}', 0)=='}'?'{':EOF;
	case '[': return _tokenize(f, buf, bufsz, ']', 0)==']'?'[':EOF;
	default: ungetc(c, f); return _tokenize(f, buf, bufsz, 0, 0)==0?buf[0]:EOF;
	}
}
\end{lstlisting}

\subsection{Формирование массива матчеров и структуры грамматики}
В результате, мы получаем очень неплохой токенизатор с достаточно богатыми возможностями, хотя, могли бы обойтись ещё более простой реализацией.
Теперь сформируем из файла массив матчеров для последующей <<сборки>>:
\begin{lstlisting}[firstnumber=34,caption=uforth.c -- Загрузка ФОРТ-программы и её интерпретация]
Grammar* grammar_uforth_load(const char* name) {
	Grammar* g;
	size_t rules_sz =16;
	size_t matchers_sz = 256;
	FILE *f = fopen(name, "r");
	if ( !f ) return (fprintf(stderr, "Cannot open grammar uforth %s\n", name), NULL);
	Matcher* allmatchers = calloc(sizeof(Matcher), matchers_sz); 
	Rule* rules = calloc(sizeof(Rule), rules_sz);
	size_t bufsz = 1024;
	char buf[bufsz];
	int r = 0;
	int m = 0;
	int ttype;
	while(1) {
		ttype = tokenize(f, buf, bufsz);
		if ( ttype == EOF) break;
		if ( ttype == -2) {fprintf(stderr, "Out of buffer when reading rule %d, matcher %d at %lu\n", r, m, ftell(f)); break;}
		if ( r >= rules_sz-1 ) rules = realloc(rules, sizeof(Rule)*(rules_sz*=2));
		if ( m >= matchers_sz-1 ) allmatchers = realloc(allmatchers, sizeof(Matcher)*(matchers_sz*=2));
		switch(ttype) {
		case '%': if ( m > 0 ) allmatchers[m++] = (Matcher)MEND; rules[r++] = (Rule){strdup(buf+1), allmatchers+m}; break;
		case '"': allmatchers[m++] = (Matcher){NULL, literal_match, 0, strdup(buf), m}; break;
		case '{': allmatchers[m++] = (Matcher){NULL, range_match, 0, strdup(buf), m}; break;
		case '[': allmatchers[m++] = (Matcher){NULL, oneof_match ,0, strdup(buf), m}; break;
		case '?': allmatchers[m++] = (Matcher){NULL, optional_match, 1, .n=m}; break;
		case '*': allmatchers[m++] = (Matcher){NULL, repeat_match, 1, .n=m}; break;
		case '@': allmatchers[m++] = (Matcher){strdup(buf+1), named_match, 1, .n=m}; break;
		case '.': allmatchers[m++] = (Matcher){NULL, any_match, 0, .n=m}; break;
		case '|': allmatchers[m++] = (Matcher){NULL, or_match, 2, .n=m}; break;
		case ';': allmatchers[m++] = (Matcher){NULL, seq_match, 2, .n=m}; break;
		case '!': allmatchers[m++] = (Matcher){NULL, not_match, 1, .n=m}; break;
		case '&': allmatchers[m++] = (Matcher){NULL, ahead_match, 1, .n=m}; break;
		default:  allmatchers[m++] = (Matcher){NULL, ref_match, 0, strdup(buf), .n=m}; break;
		}
	}
	if (ttype < -1 ) {
		fclose(f);
		fprintf(stderr, "Error while parsing\n");
		free(rules);
		free(allmatchers);
		return NULL;
	}
	fclose(f);
	allmatchers[m++] = (Matcher)MEND;
	rules[r++] = (Rule){NULL, NULL};
	fprintf(stderr, "Rules found: %d, Matchers: %d\n", r, m);
	g = assemble_grammar(rules, allmatchers, 1);
	return g;
}
\end{lstlisting}
Данная функция сначала инициализирует массивы правил {\tt rules}  и матчеров {\tt allmatchers}, которые будут, при необходимости, <<растягиваться>>. 
Затем, запускает в <<бесконечном цикле>> токенизатор и обрабатывает токены в зависимости от их типа. 
В конце каждого правила, в массив вставляется специальный <<нулевой>> матчер {\tt MEND}.

После того, как все матчеры помещены в массив, а каждое правило смотрит на серию матчеров внутри этого массива, которое относится к нему, вызывается сборщик грамматики {\tt assemble\_grammar}.  
\subsection{Сборщик грамматики}
Идею сборщика грамматики из массива матчеров мы уже рассматривали ранее (см.~\autoref{lst:assembler-sample}).
Вот его реализация:
\begin{lstlisting}[firstnumber=171, caption=packrat.c -- Сборщик грамматики]
Grammar* assemble_grammar(Rule* rules, Matcher* matchers, int allocated) {
	Grammar* g = malloc(sizeof(Grammar));
	*g = (Grammar){rules, matchers, allocated};
	for ( Rule* r = rules; r->name; r++ ) 
		if (!assemble_matcher(g, r))  return (grammar_free(g), NULL);
	return g;
}
\end{lstlisting} 
Высокоуровневый сборщик грамматики {\tt assemble\_grammar} запускает для каждого правила сборщик цепочки\\ {\tt assemble\_matcher}:
\begin{lstlisting}[firstnumber=140, caption=packrat.c -- сборщик цепочки для правила]
int assemble_matcher(Grammar* g, Rule* r) {
	Matcher* stack = NULL;
	for (Matcher* x = r->matchers; x->func; x++ ) {
		switch(x->args) {
		case 2:
			if (take_two(x, &stack)) return !printf("Cannot build sequence -- nothing on stack\n");
			break;
		case 1:
			if ( take_one(x, &stack) ) return !printf("Cannot build not\n");
			break;
		case 0:
			if (x->func == ref_match) {
				x->ref = find_rule(g, x->opt);
				if ( !x->ref ) return !printf("Rule with name %s was not found in the grammar\n", x->opt);
			}
			push(x, &stack);
		default:
			break;
		}
		x->g = g; /* set ref to g */
		//x->rule = r; /* this rule reference */
		x->cache = ht_init(match_hash, match_cmp, NULL, NULL, 16, x);
	}
	if ( stack ){
		r->assembly = stack;
		return 1;
	}
	printf("Stack is empty for rule %s\n", r->name);
	return 0;
}
\end{lstlisting}
В дополнение к операциям со стеком, данная функция также инициализирует некоторые поля матчеров: {\tt g}  -- ссылка на грамматику, и {\tt cache} -- кеш результатов.
Для матчеров типа ссылок на нетерминалы ({\tt ref\_match}), данная функция ищет соответствующее правило {\tt find\_rule}) и записывает его в поле {\tt ref} матчера.

В этой функции  мы использовали поле {\tt next} структуры {\tt Matcher}, чтобы организовать стек.
Функция {\tt push} просто связывает матчеры в односвязный список:
\begin{lstlisting}[firstnumber=56, caption=packrat.c -- push на стек]
int push(Matcher* x, Matcher** stack) {
	x->next = *stack;
	*stack = x;
	return 0;
}
\end{lstlisting} 
Ещё пара функций  {\tt take\_one}  и {\tt take\_two} снимают с вершины один или два матчера и связывают их с левым или с обоими потомками соответственно:
\begin{lstlisting}[firstnumber=40, caption=packrat.c -- снятие и связывание]
int take_two(Matcher* x, Matcher** stack) {
	if (!(*stack) ||  !(*stack)->next) return -1;
	x->left = (*stack)->next;
	x->right = (*stack);
	x->next = (*stack)->next->next;
	*stack = x;
	return 0;
}	

int take_one(Matcher* x, Matcher** stack) {
	if( !(*stack) ) return -1;
	x->left = (*stack);
	x->next = (*stack)->next;
	*stack = x;
	return 0;
}
\end{lstlisting}
Ну вот, в принципе и все приготовления для того, чтобы получить на выходе AST нашего выражения, используя только описание грамматики, парсер и компактный интерпретатор\footnote{В сумме файлы {\tt packrat.c} и {\tt uforth.c} занимают 317 строк.}
\subsection{Полный текст грамматики математических выражений}
Приведем полный текст на нашем микро-ФОРТе для формирования грамматических правил для packrat-парсера:
\begin{lstlisting}[language=ppf, caption=expr.ppf -- Грамматика в виде $\mu Forth$-записи, label=lst:final-grammar]
%EXPR SP TERM ; 
	SP [+-] @BINOP ;  SP ; TERM ; * ; 
	"" |  @EXPR
%TERM FACT SP [/*] @BINOP ; SP ; FACT ; * ; @TERM
%FACT ATOM SP ; "^" @POW SP ; ATOM ; * ; @FACT
%ATOM FUNC VAR | NUM | BRACKETS | UNARY |
%FUNC NAME @NAME "(" ; EXPR @ARG ; SP ; ")" ; @FUNC
%VAR NAME @VAR
%UNARY [+-] @UNOP UNARY ! ; ATOM ;
%NUM {09} {09} * ; "." {09} * ; ? ; @NUM
%BRACKETS "(" EXPR ; ")" ;
%SP [ \t\n] * 
%NAME {az} {AZ} | "_" | {09} {az} | {AZ} | * ;
\end{lstlisting}
Здесь  введены дополнительные конструкции, которые позволяют произвольно расставлять пробелы между операторами, числами, переменными, скобками.
Даны определения числа, переменной и прочих операторов.
Именование шаблонов происходит только в том случае, если нам требуется убрать неоднозначность или ввести приоритет.

Рекурсивный вызов {\tt match\_walk} (см.~\autoref{lst:match-walk}) для выражения \eqref{eq:sample-expr} и грамматики \autoref{lst:final-grammar} будет формировать такое дерево: \autoref{fig:match-tree}.

Здесь горизонтальными стрелками связаны последовательные обратные вызовы от одного вызова {\tt match\_walk} и вертикальными -- начало рекурсивного вызова {\tt match\_walk}, для тех элементов, которые мы считаем нетерминалами. Содержимое каждого терминального элемента указано в кавычках.

Как видно, нам удалось повысить уровень абстракции и опустить значительную часть синтаксических конструкций и оставить только необходимые элементы.
Но это дерево ещё недостаточно абстрактно, чтобы компактно записать все правила дифференцирования. 

\begin{figure}[b]
	\begin{center}
		\begin{tikzpicture}[ 
			node distance =1cm
		]
		\node (expr) at (-2,0) {EXPR};
		\node (term1) at (-2,-1) {TERM}; 
			\node (fact1) at (-2, -6) {FACT};
				\node (num1) at (-2, -7) {NUM "3"};
			\node (binop3) at (0, -6) {BINOP "*"};
			\node (fact2) at (2, -6) {FACT};
				\node (var1) at (2, -7) {VAR "a"};
		\node (binop1) at (0, -1) {BINOP "+"};
		\node (term2) at (2, -1) {TERM};
			\node (fact3) at (2, -4) {FACT};
				\node (var2) at (2, -5) {VAR "b"};
			\node (binop4) at (4, -4) {BINOP "/"};
			\node (fact4) at (6, -4) {FACT};
				\node (num2) at (6, -5) {NUM "7"};
		\node (binop2) at (4, -1) {BINOP "-"};
		\node (term3) at (6, -1) {TERM};
			\node (fact5) at (6, -2) {FACT};
				\node (var3) at (6, -3) {VAR "c"};
				\node (pow1) at (8, -3) {POW "$\wedge$"};
				\node (num3) at (10, -3) {NUM "2"};
		\path [->]  (expr) edge (term1)
						(term1) edge (binop1) 
						(binop1) edge (term2)
						(term2) edge (binop2)
						(binop2) edge (term3)
					(term1) edge (fact1)
					(fact1) edge (binop3) 
					(fact1) edge (num1)
					(term2) edge (fact3)
					(fact3) edge (binop4)
					(binop4) edge (fact4)
					(fact4) edge (num2)
					(fact3) edge (var2)
					(fact2) edge (var1)
					(term3) edge (fact5)
					(fact5) edge (var3)
					(var3) edge (pow1)
					(pow1) edge (num3)
					(binop3) edge (fact2);	
		\end{tikzpicture}
	\end{center}
\caption{Дерево синтаксического разбора выражения \eqref{eq:sample-expr} по правилам грамматики \autoref{lst:final-grammar}}
\label{fig:match-tree}
\end{figure}

Синтаксис нашего микро-Форта может показаться непривычным, но,  при желании, можно на том же микро-Форте сформировать синтаксис классической EBNF-нотации или PEG и использовать уже её для генерации парсеров.


\chapter{Метод продукций и язык правил}

Теперь двинемся немного дальше. Разберём, что мы будем дальше делать с нашим почти-AST. Начнём с фантазий на тему \emph{как} бы мы хотели обрабатывать выражения. 
Идеальным был бы вариант, близкий к записи математических правил \eqref{eq:best-form}.
Но у нас на входе дерево вида, показанного на рис.~\ref{fig:match-tree}. 
Это дерево мы можем представить в виде некоторого скобочного префиксного выражения:
\begin{equation}
\begin{split}
	(\mathrm{EXPR}\; & (\mathrm{TERM}\;(\mathrm{FACT}\; (\mathrm{NUM}\; 3))\;(\mathrm{BINOP}\; *)\; (\mathrm{FACT}\; (\mathrm{VAR}\; a))) \\
		  & (\mathrm{BINOP}\; +) \\
		  & (\mathrm{TERM}\; (\mathrm{FACT}\; (\mathrm{VAR}\; b))\;(\mathrm{BINOP}\; /) (\mathrm{FACT}\; (\mathrm{NUM}\;7))) \\
		  & (\mathrm{BINOP}\; -) \\
		  & (\mathrm{TERM}\;(\mathrm{FACT}\;(\mathrm{VAR}\;c)\;(\mathrm{POW}\;\wedge) (\mathrm{NUM}\; 2))))
\end{split} 
\end{equation}

Это, т.н. S-выражения\cite{s-expr}, основа одного из старейших языков программирования ЛИСП, его ответвления Scheme.

В отличие от обычной строки, это уже, всё-таки, некоторая структура, готовая к преобразованию.
Нам нужно придумать способ составления записи {\em правил} преобразования одного S-выражения в другое S-выражение. 
Делать мы это будем по принципам нормальных алгорифмов Маркова\cite{markov}, записывая правила в виде списка пар шаблон $\to$ структура (антецедент и консеквент --- импликация или продукция), но, для общности, расширим его, добавив {\em свободные переменные}, как в РЕФАЛ-е\cite{refal} и позволив рекурсивно вызывать обработчик правил на частях структур.

Чтобы не сильно нагружать синтаксис самих s-выражений для обозначения элементов правил, можно немного видоизменить списки, чтобы сделать два варианта: просто список и рекурсивный вызов обработчика (по аналогии с РЕФАЛ-ом). Возьмём обозначения таких списков из языка TCL\cite{tcl}: Фигурные скобки будут обозначать просто список:
\begin{verbatim}
{a b c {d e} f}
\end{verbatim}
, а квадратные -- аппликацию (повторное применение правил):
\begin{verbatim}
[Diff {+ a b}] 
\end{verbatim}

Антецедент от Консеквента будем отделять символом \verb|=>|.
Например:
\begin{verbatim}
{+ a 0} => a
\end{verbatim}
Полное описание синтаксиса правил в формате \texttt{.ppf}  (на языке $\mu Forth$) приведено в \ref{sec:rules.ppf}.
 
\subsection{Задача переворота списка}
Забегая вперед, скажу, что нам нужно будет переворачивать список, поскольку для построения дерева вычислений последовательных операций сложения и вычитания требуется, чтобы первая операция оказалась на самом нижнем уровне дерева, а на его вершине -- последняя операция.

Попробуем решить задачу превращения списка \verb|{EXPR {TERM x} {BINOP +} {TERM y} {BINOP -} {TERM z}}| в дерево с лево-ассоциативным порядком выполнения операций с использованием нашего языка правил.
Для этого нам потребуется уметь отделять голову списка от его хвоста.
Введем с этой целью символ хвоста списка \verb|...| (многоточие -- эллипсис), который будет обозначать оставшуюся часть списка в антецеденте, чтобы потом использовать его в консеквенте. Запишем правила импликации для <<функции>> \verb|REV|:
\begin{verbatim}
[REV {} _1] => _1
[REV {{BINOP _1} _2 ...} _3] => [REV {...} {_1 _3 _2}]
[REV {EXPR _1 ...}] => [REV {...} _1]  
\end{verbatim}
здесь мы использовали синтаксис \verb|_n| для обозначения свободных переменных, которые при проверке шаблона будут связываться с соответствующими элементами структуры.

Посмотрим, что будет происходить с нашим списком: 
\begin{verbatim}
[REV {EXPR {TERM x} {BINOP +} {TERM y} {BINOP -} {TERM z}}]
    => [REV {{BINOP +} {TERM y} {BINOP -} {TERM z}} {TERM x}]
    => [REV {{BINOP -} {TERM z}} {+ {TERM x} {TERM y}}]
    => [REV {} {- {+ {TERM x} {TERM y}} {TERM z}}]
    => {- {+ {TERM x} {TERM y}} {TERM z}}
\end{verbatim}
как видно, мы добились нужного результата, упразднив, к тому же, наши синтаксические подпорки вроде терма \verb|BINOP|.
 
\section{ЛИСП-список, как конструкционный принцип}
Ещё раз имеет смысл сказать, что в ЛИСП заложены важные концепции, которые делают его актуальным до сих пор.
В частности, все возможные скобочные структуры (из S-выражений) прекрасно представляются в виде элементарного строительного элемента -- {\em пары} (pair). А список представляется в виде рекуррентной структуры (см.~\autoref{fig:pairs}). Списком является либо пустой элемент (\texttt{NIL}), либо пара, состоящая из головы (head) и хвоста (tail), где Хвост также является списком.

\begin{figure}[b]
\begin{tikzpicture}[
					level 1/.style={sibling distance=3cm},
					level 2/.style={sibling distance=7cm},
					level 3/.style={sibling distance=4cm},
					level 4/.style={sibling distance=1.6cm},
					level 5/.style={sibling distance=1.4cm},
					level distance = 0.7cm]
	\node[draw] {pair}
		child{ node {EXPR} }
		child{ node[draw] {pair} 
			child { node[draw] { pair } 
				child { node {TERM}}
				child { node[draw] {pair}
					child { node {x} }
					child { node {NIL}}}}
			child { node[draw] {pair}
				child { node[draw] {pair}
					child { node {BINOP}}
					child { node[draw] {pair}
						child {node {+}}
						child {node {NIL}}}}
				child {node[draw] {pair}
					child {node[draw] {pair}
						child {node {TERM}}
						child {node[draw] {pair}
							child { node {y}}
							child {node {NIL}}}}
					child {node {NIL}}}}};
\end{tikzpicture}
\caption{S-выражение (EXPR (TERM x) (BINOP +) (TERM y)), построенное с помощью пар}
\label{fig:pairs}
\end{figure}
\subsection{Вариантный тип}
Таким образом, у нас появляется новый универсальный тип данных, который может принимать значения различных базовых типов, в том числе и pair, из которого мы будем строить S-выражения. Такой тип называется \emph{вариантным}, или \emph{variant} В языке Си это делается при помощи конструкции \texttt{union}:
\begin{lstlisting}[firstnumber=8, caption=nlisp.h -- тип variant]
typedef struct _pair pair;

typedef struct _variant variant; 

struct _variant {
	int type;
	union {
		pair* pair;
		char* str;
	};
};
struct _pair {
	variant head;
	variant tail;
};
\end{lstlisting}
Здесь мы пока ограничились только строковыми типом данных и парой, хотя, никто нам не запрещает увеличить количество вариантов хранимых типов. Основное отличие от базовых типов -- это хранение идентификатора внутреннего типа вместе со значением. Номер типа хранится в поле \texttt{type} и может принимать следующие значения:
\begin{lstlisting}[firstnumber=4, caption=nplisp.h -- идентификаторы вариантов типов]
#define VAR_NIL 0
#define VAR_PAIR 1
#define VAR_STR 2
\end{lstlisting}
Числа пока тоже будем представлять в виде строк.

\section{Базовые операции в Вариантным типом}
\subsection{Работа с парами и списками}
Из ЛИСП-а возьмем базовые операции работы со списком. А именно, взятие левой (head) и правой (tail) частей, которые исторически\cite{lisp} обозначаются как \texttt{CAR} и \texttt{CDR}. Дополним их также всеми часто используемыми комбинациями:
\begin{lstlisting}[firstnumber=25, caption=nlisp.h -- проверка типа]
#define NIL (variant){VAR_NIL, NULL}
#define ISNIL(x) ((x).type == VAR_NIL)
#define ISPAIR(x) ((x).type == VAR_PAIR)
#define ISSTR(x) ((x).type == VAR_STR)
\end{lstlisting}
\begin{lstlisting}[firstnumber=36, caption=nlisp.h -- работа со списком]
#define VARCONS(head, tail) (variant){VAR_PAIR, cons(head, tail)}
#define CAR(x) ((x).pair->head)
#define CDR(x) ((x).pair->tail)
#define CDAR(x) ((x).pair->head.pair->tail)
#define CADR(x) ((x).pair->tail.pair->head)
#define CADDR(x) CADR(CDR(x))
#define VARSTREQ(x, s) !strcmp((x).str, s)
\end{lstlisting}

\subsection{Создание строки}
Создание строки из диапазона. Оно нам потребуется для конвертации Совпадений (Match) в строки.
\begin{lstlisting}[firstnumber=6]
variant var_string_from_range(const char* start, const char* end) {
	return (variant){VAR_STR, .str=strndup(start, end-start)};
}
\end{lstlisting}
\begin{lstlisting}[firstnumber=10]
variant var_string(const char* s) {
	return (variant){VAR_STR, .str=strdup(s)};
}
\end{lstlisting}
Создание пары из двух вариантов.
\begin{lstlisting}[firstnumber=14]
pair* cons(variant head, variant tail) {
	pair* p = malloc(sizeof(pair));
	*p = (pair){head, tail};
	return p;
}
\end{lstlisting}
Клонирование варианта.
\begin{lstlisting}[firstnumber=20]
variant varclone(variant v) {
	if ( ISPAIR(v) ) return VARCONS(varclone(CAR(v)), varclone(CDR(v)));
	if ( ISSTR(v) ) return (variant){VAR_STR, .str=strdup(v.str)};
	return NIL;
}
\end{lstlisting}
Функции печати вариантного типа  -- в виде S-выражения.
\begin{lstlisting}[firstnumber=28]
static void _varprintlist(variant lst, int indent) {
	if(indent < 4) { putchar('\n'); for(int i = 0; i <indent; i++) putchar(' '); }
	printf("(");
	for (int first = 1; lst.type != VAR_NIL ; lst=CDR(lst), first=0) {
		if (!first) printf(" ");
		_varprint(CAR(lst), indent+1);
	}
	printf(")");
}

void varprintlist(variant lst) {
	_varprintlist(lst, 0);
}

static void _varprint(variant x, int indent) {
	switch(x.type) {
	case VAR_NIL: printf("NIL"); break;
	case VAR_STR: printf("%s", x.str); break;
	case VAR_PAIR: _varprintlist(x, indent); break;
	default: printf("Unknown type %d\n", x.type); break;
	}
}

void varprint(variant x) {
	_varprint(x, 0);
	putchar('\n');
}
\end{lstlisting}
Рекурсивное освобождение вариантного типа
\begin{lstlisting}[firstnumber=58]
void varfree(variant x) {
	switch(x.type) {
	case VAR_STR: free(x.str); break;
	case VAR_PAIR: varfree(CAR(x)); varfree(CDR(x)); free(x.pair); break;
	}
}
\end{lstlisting}
Поэлементное рекурсивное сравнение двух вариантов.
\begin{lstlisting}[firstnumber=65]
int vareq(variant a, variant b) {
	if (a.type != b.type ) return 0;
	if (ISNIL(a) && ISNIL(b) ) return 1;
	if (ISSTR(a) && !strcmp(a.str, b.str))  return 1;
	if (ISPAIR(a)) return vareq(CAR(a), CAR(b)) && vareq(CDR(a), CDR(b));
	return 0;
}
\end{lstlisting}
\section{Правила преобразования списков}
\subsection{Интерфейс таблицы переменных}
Для того, чтобы хранить переменные, унифицированные в процессе проверки совпадения шаблона, мы воспользуемся
простым односвязным списком, определение которого описано в файле \texttt{vartable.h}.

Таблица переменных имеет тип \verb|VarTable*| и инициализируется значением \verb|NULL|. 

Добавление варианта в таблицу по имени происходит путём вызова функции \verb|vartable_set|:
\begin{verbatim}
    VarTable* t = NULL;
    variant v = var_string("value");
    vartable_set(&t, "name", v);
\end{verbatim}

Проверка наличия значения по имени происходит через функцию \verb|vartable_get|:
\begin{verbatim}
    VarTable* t;
    variant v;
    if ( vartable_get(t, "name", &v) ) 
        varprint(v);
    else 
        printf("No such variable\n");
\end{verbatim}

После того, как таблица становится ненужна, её следует освободить функцией \verb|vartable_free|.

Стоит заметить, что данная таблица не копирует значения, копируются только имена. Соответственно, таблица должна быть освобождена \emph{после} того, как будет освобождено выражение, проверяемое в шаблоне. Функции построения выражения (билдеры), которые используют переменные из таблицы \emph{должны копировать} значения, полученные из таблицы  в свои новые выражения.

\subsection{Шаблоны соответствия}
Ниже перечислены функции шаблонов, которые определяют соответствие s-выражения шаблону. 
Все такие функции имеют общую сигнатуру: принимают на вход рассматриваемое выражение {\tt expr} и выражение шаблона {\tt pattern}, а также таблицу переменных {\tt t}. 
Функция должна вернуть \verb|1| в случае, если выражение соответствует шаблону, унифицировав переменные и 0 -- в противном случае.
\paragraph{Шаблон списка} Рекурсивно проверяет, что рассматриваемое выражение является либо пустым списком, либо парой. Соответствует конструкции \verb|{x y z}| или её синтаксическому выражению \verb|(LIST x y z)|. Если шаблон также является пустым списком (конструкция \verb|{}|), то возвращает \verb|1|, в противном случае проверяет соответствие головы выражения --- \verb|CAR(expr)| с головой шаблона  --- \verb|CAR(pattern)| и, в случае совпадения, продолжает рекурсивно.
\begin{lstlisting}[firstnumber=40]
static int list_match(variant expr, variant pattern, VarTable** t) {
	if (ISNIL(expr) && ISNIL(pattern)) return 1;
	if ((ISNIL(expr) || ISPAIR(expr)) && ISPAIR(pattern)) {
		/* check special ellipsis case */
		variant p = CAR(pattern);
		if (ISPAIR(p) && ISSTR(CAR(p)) && VARSTREQ(CAR(p), "ELLIPSIS")){ 
			vartable_set(t, CADR(p).str, expr); 
			return 1;
		} 
		if ( ISPAIR(expr) && pattern_match(CAR(expr), p, t) )
			return list_match(CDR(expr), CDR(pattern), t);
	}
	return 0;
}
\end{lstlisting}
Стоит обратить внимание, что данная функция также проверяет шаблон ELLIPSIS (конструкция \verb|...|) внутри списка (он должен быть последним в списке). В этом случае,  весь список, начиная с текущего элемента, записывается в переменную \verb|ELLIPSIS|. 
\paragraph{Шаблон аппликации} Проверяет, что элемент является аппликацией. Соответствует конструкции вида \verb|[x y z ...]| (список в квадратных скобках). Cинтаксически это s-выражение вида \verb|(APPLY x y z ELLIPSIS)|.
\begin{lstlisting}[firstnumber=55]
static int apply_match(variant expr, variant pattern, VarTable** t) {
	if (ISPAIR(expr) && ISPAIR(pattern) 
			&& ISSTR(CAR(expr))
			&& VARSTREQ(CAR(expr), "APPLY")) 
		return list_match(CDR(expr), pattern, t);
	return 0;
}
\end{lstlisting}
\paragraph{Шаблон имени} Проверяет точное соответствие имени. Соответствует конструкции вида \verb|sin| или \verb|REV| (синтаксическое s-выражение \verb|(NAME sin)|.
\begin{lstlisting}[firstnumber=63]
static int name_match(variant expr, variant pattern, VarTable** t) {
	if (ISSTR(expr) && VARSTREQ(expr, CAR(pattern).str)) return 1;
	return 0;
}
\end{lstlisting}
\paragraph{Шаблон числа} Проверяет соответствие чисел (с точностью до \verb|double|).
\begin{lstlisting}[firstnumber=68]
static int num_match(variant expr, variant pattern, VarTable** t) {
	if ( ISPAIR(expr) && ISSTR(CAR(expr))
			&& atof(CAR(expr).str) == atof(CAR(pattern).str))
		return 1;
	return 0;
}
\end{lstlisting}
\paragraph{Шаблон переменной} Проверяет, что синтаксический элемент является именем переменной. И за поминает его в таблице, либо сверяет у унифицированным.
Соответствует конструкции \verb|V_n| (s-выражение \verb|(VARM V_n)|). Это дополнительный шаблон для сокращения записи, формально можно обойтись без него, записав \verb|{VAR _n}|.
\begin{lstlisting}[firstnumber=75]
static int var_match(variant expr, variant pattern, VarTable** t) {
	if ( ISPAIR(expr) && ISSTR(CAR(expr)) && VARSTREQ(CAR(expr), "VAR")) {
		/* Check if already unified */
		variant res;
		if ( vartable_get(*t, CAR(pattern).str, &res) ) return vareq(CADR(expr), res);
		vartable_set(t, CAR(pattern).str, CADR(expr));
		return 1;
	} 
	return 0;
}
\end{lstlisting}
\paragraph{Шаблон константы} Проверяет, что синтаксический элемент является числом и запоминает его в переменную (или сравнивает с уже унифицированной).
Соответствует конструкции \verb|C_n| (s-выражение \verb|(CONSTM C_n)|). Сокращение для шаблона \verb|{NUM _n}|.
\begin{lstlisting}[firstnumber=86]
static int const_match(variant expr, variant pattern, VarTable** t) {
	if ( ISPAIR(expr) && ISSTR(CAR(expr)) && VARSTREQ(CAR(expr), "NUM")) {
		variant res;
		if ( vartable_get(*t, CAR(pattern).str, &res) ) return atof(CADR(expr).str) == atof(res.str);
		vartable_set(t, CAR(pattern).str, CADR(expr));
		return 1;
	}
	return 0;
}
\end{lstlisting}
\paragraph{Шаблон <<любой элемент>>} Просто сохраняет синтаксический элемент в переменную либо сравнивает поэлементно с уже унифицированным. Соответствует конструкции \verb|_n| (s-выражение \verb|(ANYM _n)|).
\begin{lstlisting}[firstnumber=96]
static int any_match(variant expr, variant pattern, VarTable** t) {
	variant res;
	if ( vartable_get(*t, CAR(pattern).str, &res) )  return vareq(expr, res);
	vartable_set(t, CAR(pattern).str, expr);
	return 1;
}
\end{lstlisting}
\subsection{Разбор синтаксиса правила и выбор функции шаблона}
Поскольку  
\begin{lstlisting}[firstnumber=108, caption=pattern.c -- Выбор шаблона по префиксу синтаксической конструкции]
struct pattern_matcher {
	const char* name;
	int (*matchfunc)(variant, variant, VarTable** t);
};

struct pattern_matcher pmatchers[] = {
	{"LIST", list_match}, {"APPLY", apply_match}, {"NUM", num_match},
	{"NAME", name_match}, {"CONSTM", const_match}, {"VARM", var_match},
	{"ANYM", any_match}, {"ELLIPSIS", ellipsis_match}, {NULL, NULL}
};


int pattern_match(variant expr, variant ant, VarTable** t) {
	variant head = CAR(ant);
	if ( !ISSTR(head) ) return 0;
	for (struct pattern_matcher* m = pmatchers; m->name; m++ ) 
		if ( VARSTREQ(head, m->name) ) return m->matchfunc(expr, CDR(ant), t);
	return 0;
}
\end{lstlisting}
\subsection{Применение правил}
Применение правил строится при помощи двух функций: внешней \verb|apply_rules| и вспомогательной \verb|apply_rule|. Внешняя функция в цикле вызывает вспомогательную, пока та не вернёт истинное значение, которая означает, что выражение удовлетворяет шаблону антецедента.
\label{sec:apply-rules}
\begin{lstlisting}[firstnumber=128, caption=pattern.c -- Акт применения правила -- поиск совпадения шаблона и построение выражения]
int apply_rule(variant* expr, variant rule, variant rules) {
	int res = 0;
	variant ant = CADR(rule), conseq = CADDR(rule);
	VarTable* t = NULL;
	if (pattern_match(*expr, CADR(ant), &t)) {
		varfree(*expr);
		*expr = build_expr(CADR(conseq), t, rules);
		res = 1;
	}
	vartable_free(t);
	return res;
}
\end{lstlisting}
Стоит заметить, что таблица переменных \verb|VarTable* t| создаётся и удаляется (через \verb|vartable_free|) каждый раз при попытке применить правило.

В едином акте применения правил применяется ровно одно правило, антецедент которого оказался выполненным первым.
\begin{lstlisting}[firstnumber=141, caption=pattern.c -- Применение правил]
int apply_rules(variant *expr, variant rules) {
	for ( variant rule = CDR(rules); !ISNIL(rule); rule = CDR(rule)) 
		if (apply_rule(expr, CAR(rule), rules)) return 1;
	return 0;
}
\end{lstlisting}
\subsection{Сопоставление с шаблоном и унификация}
\subsection{Генерация новых списков}
\paragraph{Вспомогательная функция построения списка} -- строит список из шаблона \texttt{bpat}, с использованием переменных из \texttt{t} и набора правил {\tt rules}.
\begin{lstlisting}[firstnumber=10]
variant _build_list(variant bpat, VarTable* t, variant rules) {
	if ( ISNIL(bpat) ) return NIL;
	else if ( ISPAIR(bpat) ) {
		variant head = CAR(bpat);
		/* special case for ellipsis */
		if ( ISPAIR(head) && ISSTR(CAR(head)) && VARSTREQ(CAR(head), "ELLIPSIS")) {
			variant res;
			if ( vartable_get(t, CADR(head).str, &res) ) return varclone(res);
			return (printf("NO Ellipsis '%s' unified\n", CADR(head).str), NIL); 
		} 
		return VARCONS(build_expr(CAR(bpat), t, rules), _build_list(CDR(bpat), t, rules));
	}
	return NIL;
}
\end{lstlisting}
\paragraph{Построение аппликации}. Строит список с префиксом \texttt{APPLY} и повторно применяет правила. Эквивалентно синтаксической конструкции \verb|[x y ...]|.
\begin{lstlisting}[firstnumber=25]
variant apply_builder(variant bpat, VarTable* t, variant rules) {
	variant res = VARCONS(var_string("APPLY"), _build_list(bpat, t, rules));
	apply_rules(&res, rules);
	return res;
}
\end{lstlisting}
\paragraph{Построение списка} Просто перевызывает вспомогательную функцию построения списка \verb${x ...}$.
\begin{lstlisting}[firstnumber=31]
variant list_builder(variant bpat, VarTable* t, variant rules) {
	return _build_list(bpat, t, rules);	
}
\end{lstlisting}
\paragraph{Простой калькулятор} Вычисляет выражения вида \verb|${+ C_1 2}| для пяти действий.
\begin{lstlisting}[firstnumber=35]
#define ISNUM(x) ISPAIR(x) && ISSTR(CAR(x)) && VARSTREQ(CAR(x), "NUM")

variant eval_builder(variant bpat, VarTable* t, variant rules) {
	// do simple evaluation
	char buf[32];
	double result, arg1, arg2;
	int done = 0;
	variant lst = _build_list(bpat, t, rules); // recursively build list and substitute variables
	if ( ISPAIR(lst) && ISSTR(CAR(lst)) ) { 
		variant head = CAR(lst), tail = CDR(lst);
		if ( ISPAIR(tail) && ISPAIR(CDR(tail)) ) {
			if ( ISNUM(CAR(tail))  && ISNUM(CADR(tail))) {
				arg1 = atof(CADR(CAR(tail)).str), arg2 = atof(CADR(CADR(tail)).str);
				if (VARSTREQ(head, "+")) (done=1, result = arg1+arg2);
				else if (VARSTREQ(head, "-")) (done=1, result = arg1-arg2);
				else if (VARSTREQ(head, "*")) (done=1, result = arg1*arg2);
				else if (VARSTREQ(head, "/")) (done=1, result = arg1/arg2);
				else if (VARSTREQ(head, "^")) (done=1, result = pow(arg1, arg2));
			}
		} else if ( ISPAIR(tail) &&  ISNUM(CAR(tail))) {
			arg1 = atof(CADR(CAR(tail)).str);
			if (VARSTREQ(head, "-")) (done=1, result = -arg1);
		}
	} 
	if (done) {
		sprintf(buf, "%lf", result);
		varfree(lst); // drop list
		return VARCONS(var_string("NUM"), VARCONS(var_string(buf), NIL));
	}
	return lst; // in all other cases return list as is
}
\end{lstlisting}
\paragraph{Построение скаляра} Вставляет значение переменной: например \verb|_1| или эллипсис \verb|...|.
\begin{lstlisting}[firstnumber=67]
variant scalar_builder(variant bpat, VarTable* t, variant rules) {
	variant res;
	if ( vartable_get(t, CAR(bpat).str,  &res) ) return varclone(res);
	return (printf("NO var '%s' unified\n", CAR(bpat).str), NIL);
}
\end{lstlisting}
\paragraph{Вставка строки как есть} Вставляет имя или число.
\begin{lstlisting}[firstnumber=73]
variant asis_builder(variant bpat, VarTable* t, variant rules) {
	return varclone(CAR(bpat));
}
\end{lstlisting}
\paragraph{Вставка числовой переменной} Отрабатывает конструкцию вида \verb|C_0|. 
\begin{lstlisting}[firstnumber=77]
variant num_builder(variant bpat, VarTable* t, variant rules) {
	variant res;
	if ( vartable_get(t, CAR(bpat).str, &res)) return VARCONS(var_string("NUM"), VARCONS(varclone(res), NIL));
	return (printf("NO const '%s' unified\n", CAR(bpat).str), NIL);
}
\end{lstlisting}
\paragraph{Вставка переменной с именем переменной} Отрабатывает конструкцию вида \verb|V_0|.
\begin{lstlisting}[firstnumber=83]
variant var_builder(variant bpat, Table* t, variant rules) {
	variant res;
	if ( vartable_get(t, CAR(bpat).str, &res)) return VARCONS(var_string("VAR"), VARCONS(varclone(res), NIL));
	return (printf("NO var '%s' unified\n", CAR(bpat).str), NIL);
}
\end{lstlisting}
\subsection{Выбор построителя по префиксу синтаксического дерева}
Функция \texttt{build\_expr} выбирает построитель по префиксу. 
Соответствие префиксов и функций-построителей хранится в таблице \texttt{builders}.
\begin{lstlisting}[firstnumber=89]
struct builder {
	const char* name;
	variant (*buildfunc)(variant, VarTable*, variant);
};
struct builder builders[] = {
	{"APPLY", apply_builder}, {"LIST", list_builder},
	{"CONSTM", num_builder}, {"VARM", var_builder},
	{"ANYM", scalar_builder}, {"ELLIPSIS", scalar_builder},
	{"NAME", asis_builder}, {"NUM", asis_builder}, {"EVAL", eval_builder},
	{NULL, NULL}};

variant build_expr(variant conseq, VarTable* t, variant rules) {
	variant head = CAR(conseq);
	for (struct builder* b = builders; b->name; b++ ) 
		if (VARSTREQ(head, b->name)) return b->buildfunc(CDR(conseq), t, rules);
	/* otherwise, copy as-is */
	return varclone(conseq);
}

\end{lstlisting}
\section{Синтаксис правил}
\label{sec:rules.ppf}
Здесь мы приводим полный файл грамматики правил
\begin{lstlisting}[language=ppf, caption=rules.ppf -- Грамматика правил на $\mu Forth$]
%RULES COMMENT RULE | "\n" ; * @RULES 
%COMMENT SP "--" ; [\n] ! . ; * ; 
%RULE ATOM @ANT SP ; "=>" ; SP ; ATOM @CONS ;  SP ; @RULE
%ATOM  LEAF APPLY | EVAL | LIST | 
%APPLY  "[" SP ; NAME ; SP ATOM ; * ; SP ; @APPLY "]" ;
%EVAL  "${" SP ; NAME ; SP ATOM ; * ; SP ; @EVAL "}" ;
%LIST  "{"  SP ATOM ; * ; SP  ; @LIST "}" ;
%SP  [ \t] * 
%LEAF  CONSTM  ANYM | VARM | CONST | ELLIPSIS | NAME | 
%CONSTM "C_" {09} ? ;  @CONSTM 
%ANYM  "_" {09} ? ;   @ANYM 
%VARM  "V_" {09} ? ; @VARM 
%CONST {09} {09} * ; "." ? ; {09} * ; @NUM
%NAME  {az} {AZ} | [+-/*^] | {az} {AZ} | * ; @NAME 
%ELLIPSIS "..." @ELLIPSIS 
\end{lstlisting}

\section{Соединяем всё воедино}
\subsection{Функция обхода дерева совпадений}
Функция \texttt{match\_to\_expr\_tree} запускает \texttt{match\_walk}(см. \ref{sec:match-walk}) и передаёт
ей в качестве коллбека функцию \texttt{tree\_builder}.
Данный коллбек преобразует последовательность поступающих на вход совпадений \verb|Match* m|  в ЛИСП-список с соответствующими префиксами.
Если префикс не атомарный (не содержится в списке \texttt{atomnames}), то \texttt{match\_walk} рекурсивно вызывается на нём и строится под-список.

Корневой список создаётся в начале функции \texttt{match\_to\_expr\_tree} и префиксируется именем корневого синтаксического элемента, который передаётся в качестве аргумента \texttt{start}.
\begin{lstlisting}[firstnumber=27, caption=diff.c -- Программа для загрузки и выполнения правил]
const char* atomnames[] = {"BINOP", "UNOP", "NAME", "NUM", "VAR", "POW", "ELLIPSIS", "CONST", "VARM", "ANYM", "CONSTM", NULL};

int tree_builder(Match* m, void* data) {
	const char** n;
	variant** ctx = (variant**)data;
	variant p = VARCONS(VARCONS(var_string(m->m->name), NIL), NIL);
	**ctx = p;
	*ctx = &CDR(p);
	for ( n  = atomnames; *n; n++ ) {
		if ( !strcmp(m->m->name, *n)) {
			CDAR(p) = VARCONS(var_string_from_range(m->start, m->end), NIL);
			return 0;
		}
	}
	variant* subctx = &CDAR(p);
	return match_walk(m, tree_builder, &subctx);

}

int match_to_expr_tree(Match* m, const char* start, variant* res) {
	*res = VARCONS(var_string(start), NIL);
	variant* ctx = &CDR(*res);
	return match_walk(m, tree_builder, &ctx);
}
\end{lstlisting}
\subsection{Загрузка и выполнение правил}
Данная функция берет выражение \verb|expr|, построенное на предыдущем этапе из структуры совпадений, путём прохода по именованным совпадениям, зачитывает список правил из файла с именем \verb|rule_file| и пытается разобрать его при помощи грамматики, зачитанной из файла \verb|rules.ppf|, вызывая функцию \verb|load_rules_and_apply|.
\begin{lstlisting}[firstnumber=69, caption=diff.c Загрузка\, парсинг и применение правил]
int transform_with_rules(variant expr, const char* rule_file) {
	Grammar* gr = grammar_uforth_load("rules.ppf");
	if ( !gr ) return -printf("Cannot assemble rule grammar\n");

	FILE* dfr = fopen(rule_file, "r");
	if ( dfr ) {
		fseek(dfr, 0, SEEK_END);
		long sz = ftell(dfr);
		char* rules = calloc(sz+1,1);
		fseek(dfr, 0, SEEK_SET);
		fread(rules, sz, 1, dfr);
		fclose(dfr);
		load_rules_and_apply(gr, rules, expr);
		free(rules);
	}
	grammar_free(gr);
	return 0;
}
\end{lstlisting}
\subsection{Применение правил}
Функция \verb|load_rules_and_apply| берёт содержимое файла, переданное в аргументе \verb|rules| и разбирает его при помощи грамматики \verb|gr|. Затем строит из него s-выражения, при помощи той же самой функции \verb|match_to_expr_tree|, которую мы использовали для трансформации \verb|Match| $\to$ \verb|variant|. Этим объясняется то, что в список \verb|atomnames| добавлены терминалы \verb|ANYM|, \verb|CONSTM|, \verb|VARM| -- чтобы функция была более универсальной и работала для синтаксических конструкций \verb|expr.ppf| и \verb|rules.ppf|.
\begin{lstlisting}[firstnumber=56, caption=diff.c Разбор правил и запуск на выражении]
int load_rules_and_apply(Grammar* gr, const char* rules, variant expr) {
	Match* mr = grammar_parse(gr, "RULES", rules);
	if ( !mr ) return -printf("Cannot parse rules\n");
	variant gres = NIL;
	match_to_expr_tree(mr, "RULES", &gres);
	varprint(gres);
	printf("Before application: \n");
	varprint(expr);
	apply_rules(&expr, gres);
	printf("Result of application: \n");
	varprint(expr);
	varfree(gres);
	varfree(expr);
	return 0;
}
\end{lstlisting}
После того, как у нас готовы разобранные \verb|expr| и правила \verb|gres|, запускается интерпретатор правил \verb|apply_rules|.

\section{Пытаемся программировать на новом языке}
A
\begin{lstlisting}[language=rules]
-- expand syntax tree to expression tree
[EXPAND {EXPR _1 ...}] =>  [REV {...} [EXPAND _1]]
[EXPAND {TERM _1 ...}] => [REV {...} [EXPAND _1]]
[EXPAND {FACT _1}] => [EXPAND _1]
[EXPAND {FUNC {NAME _1} {ARG _2}}] => {FUNC _1 [EXPAND _2]}
[EXPAND {FACT _1 {POW _2} ...}] => {^ [EXPAND _1] [EXPAND {FACT ...}]}
[EXPAND _1] => _1
-- the root rule: expand, then diff, then fold constants
{EXPR ...} => [CFOLD [D {VAR x} [EXPAND {EXPR ...}]]]
-- {EXPR ...} => [EXPAND {EXPR ...}]
-- {EXPR ...} => [CONSTUP [EXPAND {EXPR ...}]]
[D V_0 {+ _1 _2}] => {+ [D V_0 _1] [D V_0 _2]}
[D V_0 {- _1 _2}] => {- [D V_0 _1] [D V_0 _2]}
[D V_0 {* C_0 _1}] => {* C_0 [D V_0 _1]}
[D V_0 {/ _1 C_0}] => {/ [D V_0 _1] C_0}
[D V_0 {/ _1 _2}] => {/ {- {* [D V_0 _1] _2} {* [D V_0 _2] _1}} {^ _2 {NUM 2}}}
[D V_0 {* _1 _2}] => {+ {* _1 [D V_0 _2]} {* _2 [D V_0 _1]}}
[D V_0 V_0] => {NUM 1}
[D V_0 {^ _1 C_0}] => {* C_0 {* [D V_0 _1] [CFOLD {^ _1 ${- C_0 {NUM 1}}}]}}
[D V_0 {FUNC _1 _2}] => {* [DF _1 _2] [D V_0 _2]}
[DF sin _1] => {cos _1}
[DF cos _1] => {- {sin _1}}
[DF exp _1] => {exp _1}
[DF _1 _2] => {{DF _1} _2}
[D V_0 {^ _1 _2}]  => [D V_0 {{FUNC exp} {* {{FUNC log} _1} _2}}]
[REV {} _1] => _1
[REV {{BINOP _1} _2 ...} _3] => [REV {...} {_1 _3 [EXPAND _2]}]
[D V_0 C_1] => {NUM 0}
[CFOLD {+ _1 _2}] => [CONSTUP {+ [CFOLD _1] [CFOLD _2]}]
[CFOLD {- _1 _2}] => [CONSTUP {- [CFOLD _1] [CFOLD _2]}]
[CFOLD {* _1 _2}] => [CONSTUP {* [CFOLD _1] [CFOLD _2]}]
[CFOLD {/ _1 _2}] => [CONSTUP {/ [CFOLD _1] [CFOLD _2]}]
[CFOLD {^ _1 _2}] => [CONSTUP {^ [CFOLD _1] [CFOLD _2]}]
[CFOLD _1] => [CONSTUP _1]
[CONSTUP {+ C_0 C_1}] => ${+ C_0 C_1}
[CONSTUP {- C_0 C_1}] => ${- C_0 C_1}
[CONSTUP {* C_0 C_1}] => ${* C_0 C_1}
[CONSTUP {+ _1 {NUM 0}}] => [CFOLD _1]
[CONSTUP {- _1 {NUM 0}}] => [CFOLD _1]
[CONSTUP {- {NUM 0} _1}] => {- [CFOLD _1]}
[CONSTUP {* _1 {NUM 1}}] => [CFOLD _1]
[CONSTUP {^ _1 {NUM 0}}] => {NUM 1}
[CONSTUP {^ _1 {NUM 1}}] => [CFOLD _1]
[CONSTUP {^ {NUM 1} _1}] => {NUM 1}
[CONSTUP {^ {NUM 0} _1}] => {NUM 0}
[CONSTUP {+ {NUM 0} _1}] => [CFOLD _1]
[CONSTUP {* {NUM 1} _1}] => [CFOLD _1]
[CONSTUP {* {NUM 0} _1}] => {NUM 0}
[CONSTUP {/ {NUM 0} _1}] => {NUM 0}
[CONSTUP {* _1 {NUM 0}}] => {NUM 0}
[CONSTUP {+ C_0 {+ C_1 _1}}] => {+ ${+ C_0 C_1} [CONSTUP _1]}
[CONSTUP {* C_0 {* C_1 _1}}] => {* ${* C_0 C_1} [CONSTUP _1]}
[CONSTUP {+ _1 C_0}] => {+ C_0 [CFOLD _1]}
[CONSTUP {* _1 C_0}] => {* C_0 [CFOLD _1]}
[CONSTUP {+ _1 _2}] => {+ [CONSTUP _1] [CONSTUP _2]}
[CONSTUP {* _1 _2}] => {* [CONSTUP _1] [CONSTUP _2]}
[CONSTUP _1] => _1

\end{lstlisting}
\appendix
\chapter{Реализация хеш-таблицы}
\label{sec:hash-table}

Основная реализация находится в файле \texttt{hash.c}, заголовочный файл -- \texttt{hash.h}.

Собственно, хеш-таблица реализует основные операции \texttt{set/get} для пар ключ-значение и является универсальной по отношению к типу хранимых значений. 
Ключ из значения разделены. 
 Поэлементное удаление не осуществляется, да оно и не нужно.\footnote{На сколько это возможно, следуем принципу минимальной сложности. Если удаление одного элемента не требуется, то его и не нужно реализовывать. Более того, имеет смысл стараться сохранять такое свойство модели данных, как \emph{монотонность} -- модель можно только наращивать, а модификаций и удаления следует избегать.} 
 Вся таблица после использования удаляется целиком.
\subsection{Инициализация хеш-таблицы}
\label{ht-init}
За инициализацию хеш-таблицы отвечает функция \texttt{ht\_init}. 
 
\begin{lstlisting}[firstnumber=5]
typedef struct _table Table;
typedef uint64_t (*hashfunc)(const void*);
typedef int (*cmpfunc)(const void*, const void*);
typedef void* (*dupfunc)(const void*);
typedef void (*freefunc)(Table*, void*, void*);


Table* ht_init(hashfunc hf, cmpfunc cf, dupfunc df, freefunc ff, size_t sz, void* data);
\end{lstlisting}
Она принимает на вход 4 внешних функции: \texttt{hashfunc} -- функцию хеширования ключа,
 \texttt{cmpfunc} -- функцию сравнения ключей, 
 \texttt{dupfunc} -- функцию дублирования значения (ключи дублируются самостоятельно). 
 \texttt{freefunc} -- функцию освобождения памяти значения (ключи освобождаются автоматом при помощи \texttt{free}).
 
 Также \texttt{ht\_init} принимает дополнительно начальный размер таблицы \texttt{sz} и указатель на пользовательские данные \texttt{void* data}.

Внутренне хеш-таблица устроена в виде таблицы с методом разрешения коллизий путем построения списков.
Каждый элемент списка представляет из себя пару указателей ключ-значение (\texttt{k, v}), указатель на функцию освобождения значения \texttt{ff} и указатель на следующий элемент: \texttt{next}.
\begin{lstlisting}[firstnumber=6]
typedef struct _htelem {
	void* k;
	void* v;
	freefunc ff;
	struct _htelem* next;
} HTElem;
\end{lstlisting}
Сама таблица является массивом указателей на головы списков (поле \texttt{table}) и хранит все переданные функции, а также общее количество элементов, помещенных в таблицу \texttt{elems}, текущий размер таблицы \texttt{sz}. 
\begin{lstlisting}[firstnumber=13]
struct _table {
	hashfunc hf;
	cmpfunc cf;
	dupfunc df;
	freefunc ff;
	size_t elems;
	size_t sz;
	void *data;
	HTElem** table;
};
\end{lstlisting}
Сама функция инициализации является просто динамической аллокацией с заполнением полей:
\begin{lstlisting}[firstnumber=24]
Table* ht_init(hashfunc hf, cmpfunc cf, dupfunc df, freefunc ff, size_t sz, void* data) {
	Table* t = malloc(sizeof(Table));
	*t = (Table){hf, cf, df, ff, 0, sz, data, calloc(sz, sizeof(HTElem*))};
	return t;
}
\end{lstlisting}
Здесь нет проверки на возвращаемое значение \texttt{malloc/calloc}, но его следовало бы добавить.


\subsection{Функции добавления и рехешинга}.
Функция добавления в хеш-таблицу \texttt{ht\_set} принимает на вход, помимо указателя на таблицу \texttt{t}, пару
ключ-значение \texttt{k, v}, а также специальную функцию освобождения значения \texttt{ff}.
Если это значение не \texttt{NULL}, то именно она будет использоваться для освобождения элемента.

Если при добавлении окажется, что ключ уже существует, то элемент под этим ключом освобождается и заменяется на новый. При этом \texttt{ht\_set} возвращает значение {\tt 2}.
В противном случае создается новый элемент и добавляется в голову соответствующего списка.

\begin{lstlisting}[firstnumber=56]
int ht_set(Table* t, void* k, const void* v, freefunc ff) {
	HTElem* e;
	size_t h = t->hf(k) % t->sz;
	for(e = t->table[h]; e; e=e->next) {
		if ( t->cf(k, e->k) ) {
			if ( e->ff ) e->ff(t, e->k, e->v); else if ( t->ff) t->ff(t, e->k, e->v);
			e->v = t->df?t->df(v):v;
			return 2;
		}
	}
	_rehash(t);
	e = malloc(sizeof(HTElem));
	*e = (HTElem){k, t->df?t->df(v):v, ff, t->table[h]};
	t->table[h] = e;
	t->elems++;
	return 1;
}
\end{lstlisting}
При необходимости (количество элементов становится $\ge \frac{9}{10}$ от размера таблицы), делается полный рехешинг (функция {\tt \_rehash}) -- размер таблицы увеличивается в 2 раза ({\tt realloc}), новая половина обнуляется ({\tt memset}), функция пробегает по всем элементам таблицы и некоторые элементы из списков перемещаются на новые места.
\begin{lstlisting}[firstnumber=45]
static int _rehash(Table* t) {
	size_t sz = t->sz*sizeof(HTElem*);
	if ( t->elems*10 < t->sz*9 ) return 0;
	t->table = realloc(t->table, 2*sz);
	memset(t->table + t->sz, 0, sz);
	for ( size_t i = 0; i < t->sz; i++) 
		for( HTElem** e = &t->table[i]; *e; e = _move(&t->table[t->hf((*e)->k) % (t->sz*2)], e));
	t->sz *= 2;
	return 1;
}
\end{lstlisting}				
Для перемещения элементов используется функция {\tt \_move}:
\begin{lstlisting}[firstnumber=34]
static HTElem** _move(HTElem** t, HTElem** e) {
	HTElem* next = (*e)->next;
	if ( *e != *t ) { 
		(*e)->next = *t;
		*t = *e;
		*e = next;
		return e;
	}
	return &(*e)->next;
}
\end{lstlisting}
Она перемешает элемент из второго аргумента {\tt e} (он передается по указателю на указатель) в голову списка, указатель на указатель которого передается в первом аргументе {\tt t}. После перемещения, функция возвращает указатель на следующий элемент за {\tt e}.

\subsection{Получение значения по ключу}
Для получения значения по ключу используется функция {\tt ht\_get}. 
Ей передается таблица {\tt t}, ключ {\tt key} и указатель на результат {\tt res}.
\begin{lstlisting}[firstnumber=74]
int ht_get(Table* t, const void* key, void** res) {
	size_t h = t->hf(key) % t->sz;
	for ( HTElem* e = t->table[h]; e; e = e->next ) 
		if ( t->cf(key, e->k) ) return (*res = e->v, 1);
	return 0;
}
\end{lstlisting}
В случае успеха, функция возвращает {\tt 1}, и указатель на значение прописывается по адресу {\tt res}.
В противном случае, функция возвращает {\tt 0}.
\subsection{Освобождение таблицы}
Для освобождения вызывается функция {\tt ht\_free}, Если задана специализированная функция освобождения, заданная при {\tt ht\_set}, то вызывается она, в противном случае, вызывается функция освобождения, заданная при инициализации хеш-таблицы (см.~\ref{ht-init}) 
\begin{lstlisting}[firstnumber=81]
void ht_free(Table* t) {
	HTElem* e, *n;
	for(size_t i = 0; i < t->sz; i++ ) {
		for(e = t->table[i]; e;) {
			n = e->next;
			if ( e->ff ) e->ff(t, e->k, e->v); else if ( t->ff ) t->ff(t, e->k, e->v);
			free(e);
			e = n;
		}
	}
	free(t->table);
	free(t);
}
\end{lstlisting}

\chapter{Задания}
\section{Расширение функциональности}
\subsection{Пошаговое применение правил}
Добавьте возможность печати промежуточного результата после применения правил особого вида.
Добавьте в грамматику правил конструкцию, при помощи которой можно помечать правила, после применения которых можно делать промежуточный вывод.
\subsection{Числовые типы}
На данный момент, числа в типе \verb|variant| не отличаются от строк. Выделите числа в отдельный тип внутри \verb|variant| и используйте этот тип в калькуляторе. Разделите типы на целый и с плавающей точкой.
\subsection{НОД и НОК в калькуляторе выражений}
Добавьте вычисление Наибольшего общего делителя и наименьшего общего кратного для числовых констант (целых). При помощи этих функций реализуйте правила сокращения дробей в функции свёртки констант.
\section{Расширение языка}
\subsection{Добавление форматирования}
Сделайте <<функцию>> генерации строки из последовательности строк или шаблона форматирования с переменными.
Продумайте интерфейс, какие изменения нужно добавить в синтаксис правил, в \verb|pattern.c|, в \verb|builder.c|, так, чтобы можно было генерировать строки из выражения, например, в стиле \LaTeX:
\begin{verbatim}
[LaTeX {+ {* 3 4} {/ X 10}] 
    => "3\cdot 4 + \frac{X}{10}
\end{verbatim}
Не забудьте, что нужно будет ещё правильно расставить скобки, если дочерние выражения имеют более низкий приоритет, чем родительский:
\begin{lstlisting}[language=rules, caption=Пример правил расстановки скобок]
-- набор правил для слагаемых
[PARENT {+ _1 _2}] => {+ [PARENT _1] [PARENT _2]}
[PARENT {* _1 _2}] => {* [PARENF _1] [PARENF _2]}
[PARENT {^ _1 _2}] => {^ [PARENA _1] [PARENA _2]}
-- набор правил для множителей
[PARENF {+ _1 _2}] => {PAR {+ [PARENT _1] [PARENT _2]}}
[PARENF {* _1 _2}] => {* [PARENF -1] [PARENF -2]}
[PARENF {^ _1 _2}] => {^ [PARENA -1] [PARENA -2]}
-- набор правил для атомов
[PARENA {+ _1 _2}] => {PAR {+ [PARENT _1] [PARENT _2] }
[PARENA {* _1 _2}] => {PAR {* [PARENF _1] [PARENF _2] }
[PARENA {^ _1 _2}] => {^ [PARENA -1] [PARENA -2]}
\end{lstlisting}
Заметьте, что такая <<функция>> форматирования может самостоятельно обратить порядок вычислений в правильный порядок в строке и даже менять знак, например:
\begin{lstlisting}[language=rules]
[LaTeX {+ _1 _2}] => [CONCAT [LaTeX [PARENT _2]] " + " [LaTeX [PARENT _1]]]
[LaTeX {- _1 _2}] => [CONCAT [LaTeX [PARENT _1]] " - " [LaTeX [PARENN _2]]]
[PARENN {- _1 _2}] => {- [PARENT _2]  [PARENT _1]}
[PARENN {+ _1 _2}] => {PAR [PARENT _1] [PARENT _2]}
[LaTeX {PAR ...}] => [CONCAT "(" [LaTeX ...] ")"]  
\end{lstlisting}

\begin{thebibliography}{10}
\bibitem{wolfram} TODO: Wolfram Mathematica
\bibitem{maxima} TODO: Maxima
\bibitem{cyclomatic} Цикломатическая сложность (Cyclomatic complexity). Материал из Википедии — свободной энциклопедии, Февраль 2024, Доступна по адресу:  \url{https://en.wikipedia.org/wiki/Cyclomatic_complexity}
\bibitem{ford2004}  Ford, Bryan (January 2004). "Parsing Expression Grammars: A Recognition Based Syntactic Foundation" \href{https://bford.info/pub/lang/peg.pdf}{(PDF)}. Proceedings of the 31st ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages. ACM. pp. 111–122. doi:10.1145/964001.964011. ISBN 1-58113-729-X.
\bibitem{llk} Context-Free Grammar (Контекстно-свободная грамматика). Материал из Википедии — свободной энциклопедии, Февраль 2024, Доступна по адресу: \url{https://en.wikipedia.org/wiki/Context-free_grammar}
\bibitem{s-expr} \url{S-expressions.org}
\bibitem{lisp} McCarthy, John (1979-02-12). \href{http://www-formal.stanford.edu/jmc/history/lisp/lisp.html}{"History of Lisp"}.
\bibitem{markov} Нормальные Алгорифмы (Маркова). Материал из Википедии -- свободной энциклопедии, Февраль 2024: \url{https://en.wikipedia.org/wiki/Markov_algorithm}
\bibitem{refal} В.Ф.Турчин. РЕФАЛ-5. Руководство по программированию и справочник \url{http://refal.ru/rf5_frm.htm}
\bibitem{tcl} Михаил Полушкин. Язык программирования TCL \url{http://minix3.ru/docs/tcl.pdf}
\end{thebibliography}

\end{document}
